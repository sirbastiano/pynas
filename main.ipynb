{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------The batch size of the data to be loaded in the model is: 4-----------\n",
      "-----------The learning rate of the data to be loaded in the model is: 0.001-----------\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from datasets.RawClassifier.loader import RawClassifierDataModule\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define dataset module\n",
    "root_dir = '/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/RawClassifier'\n",
    "dm = RawClassifierDataModule(root_dir, batch_size=4, num_workers=2, transform=None)\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Model parameters\n",
    "max_layers = int(config.getint('NAS', 'max_layers'))\n",
    "max_iter = int(config['GA']['max_iterations'])\n",
    "# GA parameters\n",
    "n_individuals = int(config['GA']['population_size'])\n",
    "mating_pool_cutoff = float(config['GA']['mating_pool_cutoff'])\n",
    "mutation_probability = float(config['GA']['mutation_probability'])\n",
    "# Logging\n",
    "logs_directory = str(config['GA']['logs_dir_GA'])\n",
    "\n",
    "# Torch stuff\n",
    "seed = config.getint(section='Computation', option='seed')\n",
    "pl.seed_everything(seed=seed, workers=True)  # For reproducibility\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy\n",
    "num_workers = config.getint(section='Computation', option='num_workers')\n",
    "accelerator = config.get(section='Computation', option='accelerator')\n",
    "\n",
    "log_learning_rate=None\n",
    "batch_size=None\n",
    "# Get model parameters\n",
    "log_lr = log_learning_rate if log_learning_rate is not None else config.getfloat(section='Search Space', option='default_log_lr')\n",
    "\n",
    "lr = 10**log_lr\n",
    "bs = batch_size if batch_size is not None else config.getint(section='Search Space', option='default_bs')\n",
    "print(f\"-----------The batch size of the data to be loaded in the model is: {bs}-----------\")\n",
    "print(f\"-----------The learning rate of the data to be loaded in the model is: {lr}-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"segmentation\"\n",
    "\n",
    "if task == 'classification':\n",
    "    from classify import ModelConstructor\n",
    "elif task == 'segmentation':\n",
    "    from segmentify import ModelConstructor\n",
    "\n",
    "from mutation import gene_mutation\n",
    "from crossover import single_point_crossover\n",
    "from copy import deepcopy\n",
    "\n",
    "import handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population:\n",
    "    def __init__(self, n_individuals, max_layers, dm, max_parameters=100000):\n",
    "        self.dm = dm # Data module for model creation\n",
    "        \n",
    "        self.n_individuals = n_individuals\n",
    "        self.max_layers = max_layers\n",
    "        self.generation = 0\n",
    "        self.max_parameters = max_parameters\n",
    "        self.save_directory = \"./models_traced\"\n",
    "        \n",
    "        \n",
    "    def initial_poll(self):\n",
    "        \"\"\"\n",
    "        Generate the initial population of individuals.    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.population = self.create_population()\n",
    "        self._update_df()\n",
    "        self.save_dataframe()\n",
    "        self.save_population()\n",
    "\n",
    "\n",
    "    def create_random_individual(self):\n",
    "        \"\"\"\n",
    "        Create a random individual with a random number of layers.\n",
    "        \"\"\"\n",
    "        return handler.Individual(max_layers=self.max_layers)\n",
    "    \n",
    "\n",
    "    def sort_population(self):\n",
    "        \"\"\"\n",
    "        Sort the population by fitness.\n",
    "        \"\"\"\n",
    "        self.population = sorted(self.population, key=lambda individual: individual.fitness, reverse=True)\n",
    "        self.checkpoint()\n",
    "        \n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"\n",
    "        Save the current population.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.save_directory, exist_ok=True)\n",
    "        self._update_df()\n",
    "        self.save_population()\n",
    "        self.save_dataframe()\n",
    "    \n",
    "    \n",
    "    def check_individual(self, individual):\n",
    "        try:\n",
    "            model_representation, is_valid = self.build_model(individual.parsed_layers)\n",
    "            if is_valid:\n",
    "                modelSize = self.evaluate_parameters(model_representation)\n",
    "                individual.model_size = modelSize\n",
    "                \n",
    "                assert modelSize > 0, f\"Model size must be greater then zero: {modelSize} Parameters\"\n",
    "                assert modelSize < self.max_parameters, f\"Model size is too big: {modelSize} Parameters\"\n",
    "                assert modelSize is not None, f\"Model size is None...\"\n",
    "                return True # Individual is valid\n",
    "        except Exception as e:\n",
    "                print(f\"Error encountered when checking individual: {e}\")\n",
    "                return False # Individual is invalid\n",
    "\n",
    "\n",
    "    def create_population(self):\n",
    "        \"\"\"\n",
    "        Create a population of unique, valid individuals.\n",
    "\n",
    "        This function generates random individuals one by one and checks if they are valid using check_individual.\n",
    "        After each candidate is generated, duplicates are removed using remove_duplicates until the population\n",
    "        size reaches n_individuals.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unique, valid individuals.\n",
    "        \"\"\"\n",
    "        population = []\n",
    "        # Generate individuals until the population reaches n_individuals, removing duplicates along the way\n",
    "        while len(population) < self.n_individuals:\n",
    "            candidate = self.create_random_individual()  # Create a random individual\n",
    "            if self.check_individual(candidate):\n",
    "                population.append(candidate)\n",
    "            \n",
    "            population = self.remove_duplicates(population)  # Remove duplicates\n",
    "        return population\n",
    "\n",
    "\n",
    "    def elite_models(self, k_best=1):\n",
    "        \"\"\"\n",
    "        Retrieve the top k_best elite models from the current population based on fitness.\n",
    "\n",
    "        The population is sorted in descending order based on the fitness attribute of each individual.\n",
    "        This function then returns deep copies of the top k_best individuals to ensure that the\n",
    "        original models remain immutable during further operations.\n",
    "\n",
    "        Parameters:\n",
    "            k_best (int): The number of top-performing individuals to retrieve. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing deep copies of the elite individuals.\n",
    "        \"\"\"\n",
    "        sorted_pop = sorted(self, key=lambda individual: individual.fitness, reverse=True)\n",
    "        topModels = [deepcopy(sorted_pop[i]) for i in range(k_best)]\n",
    "        return topModels\n",
    "\n",
    "\n",
    "    def evolve(self, mating_pool_cutoff=0.5, mutation_probability=0.85, k_best=1, n_random=3):\n",
    "        \"\"\"\n",
    "        Generates a new population ensuring that the total number of individuals equals pop.n_individuals.\n",
    "        \n",
    "        Parameters:\n",
    "            pop                  : List or collection of individuals. Assumed to have attributes: \n",
    "                                .n_individuals and .generation.\n",
    "            mating_pool_cutoff   : Fraction determining the size of the mating pool (top percent of individuals).\n",
    "            mutation_probability : The probability to use during mutation.\n",
    "            k_best               : The number of best individuals from the current population to retain.\n",
    "        \n",
    "        Returns:\n",
    "            new_population: A list representing the new generation of individuals.\n",
    "            \n",
    "        Note:\n",
    "            Assumes that helper functions single_point_crossover(), mutation(), and create_random_individual() exist.\n",
    "        \"\"\"\n",
    "        new_population = []\n",
    "        self.generation += 1\n",
    "        self.topModels = self.elite_models(k_best=k_best)\n",
    "\n",
    "\n",
    "        # 2. Create the mating pool based on the cutoff from the sorted population\n",
    "        sorted_pop = sorted(self, key=lambda individual: individual.fitness, reverse=True)\n",
    "        mating_pool = sorted_pop[:int(np.floor(mating_pool_cutoff * self.n_individuals))].copy()\n",
    "        assert len(mating_pool) > 0, \"Mating pool is empty.\"\n",
    "        \n",
    "        # Generate offspring until reaching the desired population size\n",
    "        while len(new_population) < self.n_individuals - n_random - k_best:\n",
    "            try:\n",
    "                parent1 = np.random.choice(mating_pool)\n",
    "                parent2 = np.random.choice(mating_pool)\n",
    "                assert parent1.parsed_layers != parent2.parsed_layers, \"Parents are the same individual.\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting parents: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # a) Crossover:\n",
    "            children = single_point_crossover([parent1, parent2])\n",
    "            # b) Mutation:\n",
    "            mutated_children = gene_mutation(children, mutation_probability)\n",
    "            # c) Random choice of one of the mutated children\n",
    "            for kid in mutated_children:\n",
    "                kid.reset()\n",
    "                if self.check_individual(kid):\n",
    "                    new_population.append(kid)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        # 3. Add random individuals to the new population\n",
    "        while len(new_population) < self.n_individuals - k_best:\n",
    "            try:\n",
    "                individual = self.create_random_individual()\n",
    "                model_representation, is_valid = self.build_model(individual.parsed_layers)\n",
    "                if is_valid:\n",
    "                    individual.model_size = int(self.evaluate_parameters(model_representation))\n",
    "                    assert individual.model_size > 0, f\"Model size is {individual.model_size}\"\n",
    "                    assert individual.model_size < self.max_parameters, f\"Model size is {individual.model_size}\"\n",
    "                    assert individual.model_size is not None, f\"Model size is None\"\n",
    "                    new_population.append(individual)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered when evolving population: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # 4. Add the best individuals from the previous generation\n",
    "        new_population.extend(self.topModels)\n",
    "       \n",
    "\n",
    "        assert len(new_population) == self.n_individuals, f\"Population size is {len(new_population)}, expected {self.n_individuals}\"\n",
    "        self.population = new_population\n",
    "        self._update_df()\n",
    "        self.save_dataframe()\n",
    "        self.save_population()\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.population[index]\n",
    "\n",
    "\n",
    "    def remove_duplicates(self, population):\n",
    "        \"\"\"\n",
    "        Remove duplicates from the given population by replacing duplicates with newly generated unique individuals.\n",
    "\n",
    "        Parameters:\n",
    "            population (list): A list of individuals in the population.\n",
    "\n",
    "        Returns:\n",
    "            list: The updated population with duplicates removed.\n",
    "        \"\"\"\n",
    "        unique_architectures = set()\n",
    "        updated_population = []\n",
    "\n",
    "        for individual in population:\n",
    "            # Use the 'architecture' attribute if available, otherwise fallback to a default representation.\n",
    "            arch = getattr(individual, 'architecture', None)\n",
    "            if arch is None:\n",
    "                # If no architecture attribute, use parsed_layers as unique identifier.\n",
    "                arch = str(individual.parsed_layers)\n",
    "\n",
    "            if arch not in unique_architectures:\n",
    "                unique_architectures.add(arch)\n",
    "                updated_population.append(individual)\n",
    "            else:\n",
    "                # Try to generate a unique individual up to 50 times\n",
    "                for _ in range(50):\n",
    "                    new_individual = handler.Individual(max_layers=self.max_layers)\n",
    "                    new_arch = getattr(new_individual, 'architecture', None)\n",
    "                    if new_arch is None:\n",
    "                        new_arch = str(new_individual.parsed_layers)\n",
    "\n",
    "                    if new_arch not in unique_architectures:\n",
    "                        unique_architectures.add(new_arch)\n",
    "                        updated_population.append(new_individual)\n",
    "                        break\n",
    "                else:\n",
    "                    # After 50 attempts, keep the original duplicate as a fallback.\n",
    "                    updated_population.append(individual)\n",
    "        return updated_population\n",
    "        \n",
    "    \n",
    "    def build_encoder(self, parsed_layers):\n",
    "        \"\"\"\n",
    "        Build and set the encoder based on the provided parsed layers.\n",
    "\n",
    "        This function creates an encoder using a generic network from the handler's\n",
    "        generic_network module. The encoder is configured by using the input shape\n",
    "        and number of classes from the associated data module (dm). The resulting\n",
    "        encoder is stored as an attribute of the population instance.\n",
    "\n",
    "        Parameters:\n",
    "            parsed_layers (list): A list containing configurations for each layer of the network.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        encoder = handler.generic_network.GenericNetwork(\n",
    "            parsed_layers, \n",
    "            input_channels=self.dm.input_shape[0], \n",
    "            input_height=self.dm.input_shape[1], \n",
    "            input_width=self.dm.input_shape[2], \n",
    "            num_classes=self.dm.num_classes,\n",
    "        )\n",
    "        \n",
    "        return encoder\n",
    "    \n",
    "    def build_model(self, parsed_layers):\n",
    "        \"\"\"\n",
    "        Build a model based on the provided parsed layers.\n",
    "\n",
    "        This function creates an encoder using the parsed layers and constructs a model by combining\n",
    "        the encoder with a head layer via the ModelConstructor. The constructed model is built to\n",
    "        process inputs defined by the data module (dm).\n",
    "\n",
    "        Parameters:\n",
    "            parsed_layers: The parsed architecture configuration used by the encoder to build the network.\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch model constructed with the encoder and head layer.\n",
    "        \"\"\"\n",
    "        encoder = self.build_encoder(parsed_layers)\n",
    "        constructed_model = ModelConstructor(encoder, dm).model\n",
    "        valid = ModelConstructor(encoder, dm).valid_model\n",
    "        return constructed_model, valid\n",
    "    \n",
    "    \n",
    "    def evaluate_parameters(self, model):\n",
    "        \"\"\"\n",
    "        Calculate the total number of parameters of the given model.\n",
    "\n",
    "        Parameters:\n",
    "            model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of parameters.\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        return num_params\n",
    "    \n",
    "    \n",
    "    def _update_df(self):\n",
    "        \"\"\"\n",
    "        Create a DataFrame from the population.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the population.\n",
    "        \"\"\"\n",
    "        columns = [\"Generation\", \"Layers\", \"Fitness\", \"Metric\", \"FPS\", \"Params\"]\n",
    "        data = []\n",
    "        for individual in self.population:\n",
    "            generation = self.generation\n",
    "            parsed_layers = individual.parsed_layers\n",
    "            fitness = individual.fitness\n",
    "            iou = individual.iou\n",
    "            fps = individual.fps\n",
    "            model_size = individual.model_size\n",
    "            data.append([generation, parsed_layers, fitness, iou, fps, model_size])\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns).sort_values(by=\"Fitness\", ascending=False)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        self.df = df\n",
    "    \n",
    "    \n",
    "    def save_dataframe(self):\n",
    "        \"\"\"\n",
    "        Save the DataFrame containing the population statistics to a pickle file.\n",
    "\n",
    "        The DataFrame is saved at a path that includes the current generation number.\n",
    "        In case of an error during saving, the exception details are printed.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        path = f'{self.save_directory}/src/df_population_{self.generation}.pkl'\n",
    "        try:\n",
    "            self.df.to_pickle(path)\n",
    "            print(f\"DataFrame saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving DataFrame to {path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_dataframe(self, generation):\n",
    "        path = f'./models_traced/src/df_population_{generation}.pkl'\n",
    "        try:\n",
    "            df = pd.read_pickle(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DataFrame from {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def save_population(self):\n",
    "        path = f'./models_traced/src/population_{self.generation}.pkl'\n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(self.population, f)\n",
    "            print(f\"Population saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving population to {path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_population(self, generation):\n",
    "        path = f'./models_traced/src/population_{generation}.pkl'\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                population = pickle.load(f)\n",
    "            return population\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading population from {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.population)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = Population(4, max_layers, dm=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'ReLU'}\n",
      "{'layer_type': 'AvgPool'}\n",
      "{'layer_type': 'MBConv', 'expansion_factor': '3', 'activation': 'ReLU'}\n",
      "{'layer_type': 'AvgPool'}\n",
      "{'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}\n",
      "{'layer_type': 'AvgPool'}\n",
      "{'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 6, 'activation': 'GELU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}\n",
      "{'layer_type': 'AvgPool'}\n",
      "{'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 4, 'activation': 'GELU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'MBConv', 'expansion_factor': '3', 'activation': 'ReLU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 11, 'activation': 'GELU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'ConvSE', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'GELU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}\n",
      "{'layer_type': 'MaxPool'}\n",
      "{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}\n",
      "{'layer_type': 'AvgPool'}\n"
     ]
    }
   ],
   "source": [
    "ind = handler.Individual(max_layers=12)\n",
    "parsed_layers = ind.parsed_layers\n",
    "for l in parsed_layers:\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Architecture is valid, total parameters: 34904 ***\n"
     ]
    }
   ],
   "source": [
    "encoder = pop.build_encoder(parsed_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): ConvAct(\n",
       "      (0): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): AvgPool(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (2): ResNetBlock(\n",
       "      (main_path): ResNetBasicBlock(\n",
       "        (steps): Sequential(\n",
       "          (0): ConvBnAct(\n",
       "            (0): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvBnAct(\n",
       "            (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): ConvBnAct(\n",
       "            (0): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): AvgPool(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (4): DenseNetBlock(\n",
       "      (block): Sequential(\n",
       "        (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(16, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): AvgPool(\n",
       "      (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (6): ResNetBlock(\n",
       "      (main_path): ResNetBasicBlock(\n",
       "        (steps): Sequential(\n",
       "          (0): ConvBnAct(\n",
       "            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (1): ConvBnAct(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU()\n",
       "          )\n",
       "          (2): ConvBnAct(\n",
       "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): GELU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): MaxPool(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = 4\n",
    "pop = Population(10, max_layers, dm=dm)\n",
    "pop.initial_poll()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for individual in pop:\n",
    "    individual.fitness = np.random.rand() # simulate training\n",
    "pop._update_df()\n",
    "pop.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.evolve(mating_pool_cutoff=0.5, mutation_probability=0.85, k_best=1, n_random=3)\n",
    "pop.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.topModels[0].fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model fresh created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASTrainer:\n",
    "    def __init__(self, population, idx, dm, lr, max_epochs=10):\n",
    "        self.population = population\n",
    "        self.idx = idx\n",
    "        self.dm = dm\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        # Build the model from the selected individual.\n",
    "        layers = self.population[self.idx].parsed_layers\n",
    "        self.constructed_model, is_valid = self.population.build_model(layers)\n",
    "        if not is_valid:\n",
    "            raise ValueError(\"Constructed model is not valid.\")\n",
    "        \n",
    "        self.LM = handler.GenericLightningNetwork(\n",
    "            model=self.constructed_model,\n",
    "            num_classes=self.dm.num_classes,\n",
    "            learning_rate=self.lr,\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        # Train the lightning model\n",
    "        self.trainer.fit(self.LM, self.dm)\n",
    "        self.results = self.trainer.test(self.LM, self.dm)\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_model(self, save_torchscript=True, \n",
    "                   ts_save_path=None,\n",
    "                   save_standard=True, \n",
    "                   std_save_path=None):\n",
    "        # Use generation attribute from the Population object.\n",
    "        gen = self.population.generation\n",
    "        \n",
    "        if ts_save_path is None:\n",
    "            ts_save_path = f\"models_traced/generation_{gen}/model_and_architecture_{self.idx}.pt\"\n",
    "        if std_save_path is None:\n",
    "            std_save_path = f\"models_traced/generation_{gen}/model_{self.idx}.pth\"\n",
    "        \n",
    "        # Save the results to a text file.\n",
    "        with open(f\"models_traced/generation_{gen}/results_model_{self.idx}.txt\", \"w\") as f:\n",
    "            f.write(\"Test Results:\\n\")\n",
    "            for key, value in self.results[0].items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        # Prepare dummy input from dm.input_shape\n",
    "        input_shape = self.dm.input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            input_shape = (1,) + input_shape\n",
    "        device = next(self.LM.parameters()).device\n",
    "        example_input = torch.randn(*input_shape).to(device)\n",
    "        \n",
    "        self.LM.eval()  # set the model to evaluation mode\n",
    "        \n",
    "        if save_torchscript:\n",
    "            traced_model = torch.jit.trace(self.LM.model, example_input)\n",
    "            traced_model.save(ts_save_path)\n",
    "            print(f\"Scripted (TorchScript) model saved at {ts_save_path}\")\n",
    "        \n",
    "        if save_standard:\n",
    "            # Retrieve architecture code from the individual.\n",
    "            arch_code = self.population[self.idx].architecture\n",
    "            save_dict = {\"state_dict\": self.LM.model.state_dict()}\n",
    "            if arch_code is not None:\n",
    "                save_dict[\"architecture_code\"] = arch_code\n",
    "            torch.save(save_dict, std_save_path)\n",
    "            print(f\"Standard model saved at {std_save_path}\")\n",
    "\n",
    "\n",
    "from myFit import FitnessEvaluator\n",
    "evaluator = FitnessEvaluator()\n",
    "\n",
    "# Train the models in the population           \n",
    "for idx in range(len(pop)): \n",
    "    nt = NASTrainer(population=pop, idx=idx, dm=dm, lr=1e-3, max_epochs=2)\n",
    "    nt.train()\n",
    "    nt.save_model()\n",
    "    \n",
    "    \n",
    "    # API to update the population with the results from the model training\n",
    "    result = # caller_api(nt.model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update the population with the results from the model training\n",
    "    fps = nt.results[0]['fps']\n",
    "    metric = nt.results[0]['test_mcc']\n",
    "    ####\n",
    "    pop[idx].iou = nt.results[0]['test_mcc']\n",
    "    pop[idx].fps = nt.results[0]['fps']\n",
    "    \n",
    "    pop[idx].fitness = evaluator.weighted_sum_exponential(fps, metric)\n",
    "    \n",
    "    pop.df.loc[idx, 'Fitness'] = pop[idx].fitness\n",
    "    pop.df.loc[idx, 'Metric'] = pop[idx].iou\n",
    "    pop.df.loc[idx, 'FPS'] = pop[idx].fps\n",
    "    \n",
    "    pop.save_dataframe()\n",
    "    pop.save_population()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making new generation, some important things:\n",
    "\n",
    "- check model size below the thresh when creating new child, otherwise go for another tentative.\n",
    "- retain best K model at each generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "# new_pop = generate_new_population(pop, mating_pool_cutoff, mutation_probability, k_best=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_population[2].parsed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[2].parsed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" * 20)\n",
    "print(f\"*** GENERATION {t} ***\")\n",
    "new_population = []\n",
    "\n",
    "# Create a mating pool\n",
    "mating_pool = population[:int(np.floor(mating_pool_cutoff * len(population)))].copy()\n",
    "for i in range(int(np.ceil((1 - mating_pool_cutoff) * len(population)))):\n",
    "    temp_individual = handler.Individual(max_layers=max_layers)\n",
    "    mating_pool.append(temp_individual)\n",
    "\n",
    "# Coupling and mating\n",
    "couple_i = 0\n",
    "while couple_i < len(mating_pool):\n",
    "    parents = [mating_pool[couple_i], mating_pool[couple_i + 1]]\n",
    "    children = single_point_crossover(parents=parents)\n",
    "    children = mutation(children=children, mutation_probability=mutation_probability )\n",
    "    new_population = new_population + children\n",
    "    couple_i += 2\n",
    "\n",
    "# Update the population\n",
    "population = new_population.copy()\n",
    "for i in population:\n",
    "    i.architecture = i.chromosome2architecture(i.chromosome)\n",
    "population = remove_duplicates(population=population, max_layers=max_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved TorchScript model and test with a dummy input.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_path = \"model_and_architecture.pt\"\n",
    "loaded_model = torch.jit.load(save_path, map_location=device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Ensure input is moved to the correct device\n",
    "example_input = torch.randn(1, *dm.input_shape).to(device)\n",
    "example_input = example_input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(example_input)\n",
    "print(\"Output from the loaded model:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
