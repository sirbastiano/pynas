{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pynattas as pnas\n",
    "from pynattas import classes, functions\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "from datasets.RawClassifier.loader import RawClassifierDataModule, RawClassifierDataset\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define dataset module\n",
    "root_dir = '/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/RawClassifier'\n",
    "dm = RawClassifierDataModule(root_dir, batch_size=4, num_workers=2, transform=None)\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Model parameters\n",
    "max_layers = int(config.getint('NAS', 'max_layers'))\n",
    "max_iter = int(config['GA']['max_iterations'])\n",
    "# GA parameters\n",
    "n_individuals = int(config['GA']['population_size'])\n",
    "mating_pool_cutoff = float(config['GA']['mating_pool_cutoff'])\n",
    "mutation_probability = float(config['GA']['mutation_probability'])\n",
    "# Logging\n",
    "logs_directory = str(config['GA']['logs_dir_GA'])\n",
    "\n",
    "# Torch stuff\n",
    "seed = config.getint(section='Computation', option='seed')\n",
    "pl.seed_everything(seed=seed, workers=True)  # For reproducibility\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy\n",
    "num_workers = config.getint(section='Computation', option='num_workers')\n",
    "accelerator = config.get(section='Computation', option='accelerator')\n",
    "\n",
    "log_learning_rate=None\n",
    "batch_size=None\n",
    "# Get model parameters\n",
    "log_lr = log_learning_rate if log_learning_rate is not None else config.getfloat(section='Search Space', option='default_log_lr')\n",
    "\n",
    "lr = 10**log_lr\n",
    "bs = batch_size if batch_size is not None else config.getint(section='Search Space', option='default_bs')\n",
    "print(f\"-----------The batch size of the data to be loaded in the model is: {bs}-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConstructor:\n",
    "    def __init__(self, encoder, dm, verbose=False):\n",
    "        # Validate that dm has the necessary attributes.\n",
    "        if not hasattr(dm, \"num_classes\") or not hasattr(dm, \"input_shape\"):\n",
    "            raise ValueError(\"dm must have 'num_classes' and 'input_shape' attributes.\")\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.num_classes = dm.num_classes\n",
    "        self.input_shape = dm.input_shape\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(f\"Input shape: {self.input_shape}\")\n",
    "        \n",
    "        # Verify that encoder has parameters.\n",
    "        try:\n",
    "            next(self.encoder.parameters())\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Encoder appears to have no parameters.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Provided encoder does not follow expected API.\") from e\n",
    "\n",
    "        # Validate input_shape is a tuple and properly dimensioned.\n",
    "        if not isinstance(self.input_shape, tuple):\n",
    "            raise TypeError(\"input_shape must be a tuple.\")\n",
    "        if len(self.input_shape) == 3:\n",
    "            if self.verbose:\n",
    "                print(\"Adding channel dimension to input shape.\")\n",
    "                print(f\"Original input shape: {self.input_shape}\")\n",
    "            self.input_shape = (1,) + self.input_shape\n",
    "            if self.verbose:\n",
    "                print(f\"Updated input shape: {self.input_shape}\")\n",
    "        elif len(self.input_shape) != 4:\n",
    "            raise ValueError(\"input_shape must be of length 3 or 4.\")\n",
    "\n",
    "        self.head_layer = self.build_head(input_shape=self.input_shape)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.encoder,\n",
    "            self.head_layer\n",
    "        )\n",
    "        \n",
    "        self.valid_model = self.dummy_test()\n",
    "\n",
    "    def build_head(self, input_shape=(1, 2, 256, 256)):\n",
    "        # Get the device from the encoder's parameters.\n",
    "        try:\n",
    "            device = next(self.encoder.parameters()).device\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Unable to determine device from encoder parameters.\") from e\n",
    "        \n",
    "        # Run a dummy input through the encoder to get the feature shape.\n",
    "        dummy = torch.randn(*input_shape).float().to(device)\n",
    "        try:\n",
    "            features = self.encoder(dummy)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Error when running dummy input through encoder.\") from e\n",
    "        \n",
    "        if not isinstance(features, torch.Tensor):\n",
    "            raise TypeError(\"Encoder output should be a torch.Tensor.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Feature map shape from the feature extractor:\", features.shape)\n",
    "\n",
    "        # Check that the features tensor has at least 2 dimensions.\n",
    "        if features.dim() < 2:\n",
    "            raise ValueError(\"Encoded features should have at least 2 dimensions.\")\n",
    "        \n",
    "        # Determine the number of channels from the dummy output.\n",
    "        feature_channels = features.shape[1]\n",
    "\n",
    "        # Build the head layer.\n",
    "        head_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_channels, self.num_classes)\n",
    "        )\n",
    "        if self.verbose:\n",
    "            print(\"Constructed head layer:\", head_layer)\n",
    "        return head_layer\n",
    "    \n",
    "    \n",
    "    def dummy_test(self):\n",
    "        try:\n",
    "            device = next(self.encoder.parameters()).device\n",
    "            dummy = torch.randn(*self.input_shape).float().to(device)\n",
    "            output = self.model(dummy)\n",
    "            if self.verbose:\n",
    "                print(\"Network test passed. Output shape from the model:\", output.shape)\n",
    "            \n",
    "            if not isinstance(output, torch.Tensor):\n",
    "                raise TypeError(\"Output of the model should be a torch.Tensor.\")\n",
    "            \n",
    "            if output.shape[0] != dummy.shape[0]:\n",
    "                raise ValueError(\"Batch size mismatch between input and output.\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(\"An error occurred during dummy_test:\", e)\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor.\")\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutation import gene_mutation\n",
    "from crossover import single_point_crossover\n",
    "from copy import deepcopy\n",
    "\n",
    "class Population:\n",
    "    def __init__(self, n_individuals, max_layers, dm, max_parameters=100000):\n",
    "        self.dm = dm # Data module for model creation\n",
    "        \n",
    "        self.n_individuals = n_individuals\n",
    "        self.max_layers = max_layers\n",
    "        self.generation = 0\n",
    "        self.max_parameters = max_parameters\n",
    "        self.save_directory = \"./models_traced\"\n",
    "        \n",
    "        \n",
    "    def initial_poll(self):\n",
    "        \"\"\"\n",
    "        Generate the initial population of individuals.    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.population = self.create_population()\n",
    "        self._update_df()\n",
    "        self.save_dataframe()\n",
    "        self.save_population()\n",
    "\n",
    "\n",
    "    def create_random_individual(self):\n",
    "        \"\"\"\n",
    "        Create a random individual with a random number of layers.\n",
    "        \"\"\"\n",
    "        return classes.Individual(max_layers=self.max_layers)\n",
    "    \n",
    "\n",
    "    def sort_population(self):\n",
    "        \"\"\"\n",
    "        Sort the population by fitness.\n",
    "        \"\"\"\n",
    "        self.population = sorted(self.population, key=lambda individual: individual.fitness, reverse=True)\n",
    "        self.checkpoint()\n",
    "        \n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"\n",
    "        Save the current population.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.save_directory, exist_ok=True)\n",
    "        self._update_df()\n",
    "        self.save_population()\n",
    "        self.save_dataframe()\n",
    "    \n",
    "    \n",
    "    def check_individual(self, individual):\n",
    "        try:\n",
    "            model_representation, is_valid = self.build_model(individual.parsed_layers)\n",
    "            if is_valid:\n",
    "                modelSize = self.evaluate_parameters(model_representation)\n",
    "                individual.model_size = modelSize\n",
    "                \n",
    "                assert modelSize > 0, f\"Model size must be greater then zero: {modelSize} Parameters\"\n",
    "                assert modelSize < self.max_parameters, f\"Model size is too big: {modelSize} Parameters\"\n",
    "                assert modelSize is not None, f\"Model size is None...\"\n",
    "                return True # Individual is valid\n",
    "        except Exception as e:\n",
    "                print(f\"Error encountered when checking individual: {e}\")\n",
    "                return False # Individual is invalid\n",
    "\n",
    "\n",
    "    def create_population(self):\n",
    "        \"\"\"\n",
    "        Create a population of unique, valid individuals.\n",
    "\n",
    "        This function generates random individuals one by one and checks if they are valid using check_individual.\n",
    "        After each candidate is generated, duplicates are removed using remove_duplicates until the population\n",
    "        size reaches n_individuals.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unique, valid individuals.\n",
    "        \"\"\"\n",
    "        population = []\n",
    "        # Generate individuals until the population reaches n_individuals, removing duplicates along the way\n",
    "        while len(population) < self.n_individuals:\n",
    "            candidate = self.create_random_individual()  # Create a random individual\n",
    "            if self.check_individual(candidate):\n",
    "                population.append(candidate)\n",
    "            \n",
    "            population = self.remove_duplicates(population)  # Remove duplicates\n",
    "        return population\n",
    "\n",
    "\n",
    "    def elite_models(self, k_best=1):\n",
    "        \"\"\"\n",
    "        Retrieve the top k_best elite models from the current population based on fitness.\n",
    "\n",
    "        The population is sorted in descending order based on the fitness attribute of each individual.\n",
    "        This function then returns deep copies of the top k_best individuals to ensure that the\n",
    "        original models remain immutable during further operations.\n",
    "\n",
    "        Parameters:\n",
    "            k_best (int): The number of top-performing individuals to retrieve. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing deep copies of the elite individuals.\n",
    "        \"\"\"\n",
    "        sorted_pop = sorted(self, key=lambda individual: individual.fitness, reverse=True)\n",
    "        topModels = [deepcopy(sorted_pop[i]) for i in range(k_best)]\n",
    "        return topModels\n",
    "\n",
    "\n",
    "    def evolve(self, mating_pool_cutoff=0.5, mutation_probability=0.85, k_best=1, n_random=3):\n",
    "        \"\"\"\n",
    "        Generates a new population ensuring that the total number of individuals equals pop.n_individuals.\n",
    "        \n",
    "        Parameters:\n",
    "            pop                  : List or collection of individuals. Assumed to have attributes: \n",
    "                                .n_individuals and .generation.\n",
    "            mating_pool_cutoff   : Fraction determining the size of the mating pool (top percent of individuals).\n",
    "            mutation_probability : The probability to use during mutation.\n",
    "            k_best               : The number of best individuals from the current population to retain.\n",
    "        \n",
    "        Returns:\n",
    "            new_population: A list representing the new generation of individuals.\n",
    "            \n",
    "        Note:\n",
    "            Assumes that helper functions single_point_crossover(), mutation(), and create_random_individual() exist.\n",
    "        \"\"\"\n",
    "        new_population = []\n",
    "        self.generation += 1\n",
    "        self.topModels = self.elite_models(k_best=k_best)\n",
    "\n",
    "\n",
    "        # 2. Create the mating pool based on the cutoff from the sorted population\n",
    "        sorted_pop = sorted(self, key=lambda individual: individual.fitness, reverse=True)\n",
    "        mating_pool = sorted_pop[:int(np.floor(mating_pool_cutoff * self.n_individuals))].copy()\n",
    "        assert len(mating_pool) > 0, \"Mating pool is empty.\"\n",
    "        \n",
    "        # Generate offspring until reaching the desired population size\n",
    "        while len(new_population) < self.n_individuals - n_random - k_best:\n",
    "            try:\n",
    "                parent1 = np.random.choice(mating_pool)\n",
    "                parent2 = np.random.choice(mating_pool)\n",
    "                assert parent1.parsed_layers != parent2.parsed_layers, \"Parents are the same individual.\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting parents: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # a) Crossover:\n",
    "            children = single_point_crossover([parent1, parent2])\n",
    "            # b) Mutation:\n",
    "            mutated_children = gene_mutation(children, mutation_probability)\n",
    "            # c) Random choice of one of the mutated children\n",
    "            for kid in mutated_children:\n",
    "                kid.reset()\n",
    "                if self.check_individual(kid):\n",
    "                    new_population.append(kid)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        # 3. Add random individuals to the new population\n",
    "        while len(new_population) < self.n_individuals - k_best:\n",
    "            try:\n",
    "                individual = self.create_random_individual()\n",
    "                model_representation, is_valid = self.build_model(individual.parsed_layers)\n",
    "                if is_valid:\n",
    "                    individual.model_size = int(self.evaluate_parameters(model_representation))\n",
    "                    assert individual.model_size > 0, f\"Model size is {individual.model_size}\"\n",
    "                    assert individual.model_size < self.max_parameters, f\"Model size is {individual.model_size}\"\n",
    "                    assert individual.model_size is not None, f\"Model size is None\"\n",
    "                    new_population.append(individual)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered when evolving population: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # 4. Add the best individuals from the previous generation\n",
    "        new_population.extend(self.topModels)\n",
    "       \n",
    "\n",
    "        assert len(new_population) == self.n_individuals, f\"Population size is {len(new_population)}, expected {self.n_individuals}\"\n",
    "        self.population = new_population\n",
    "        self._update_df()\n",
    "        self.save_dataframe()\n",
    "        self.save_population()\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.population[index]\n",
    "\n",
    "\n",
    "    def remove_duplicates(self, population):\n",
    "        \"\"\"\n",
    "        Remove duplicates from the given population by replacing duplicates with newly generated unique individuals.\n",
    "\n",
    "        Parameters:\n",
    "            population (list): A list of individuals in the population.\n",
    "\n",
    "        Returns:\n",
    "            list: The updated population with duplicates removed.\n",
    "        \"\"\"\n",
    "        unique_architectures = set()\n",
    "        updated_population = []\n",
    "\n",
    "        for individual in population:\n",
    "            # Use the 'architecture' attribute if available, otherwise fallback to a default representation.\n",
    "            arch = getattr(individual, 'architecture', None)\n",
    "            if arch is None:\n",
    "                # If no architecture attribute, use parsed_layers as unique identifier.\n",
    "                arch = str(individual.parsed_layers)\n",
    "\n",
    "            if arch not in unique_architectures:\n",
    "                unique_architectures.add(arch)\n",
    "                updated_population.append(individual)\n",
    "            else:\n",
    "                # Try to generate a unique individual up to 50 times\n",
    "                for _ in range(50):\n",
    "                    new_individual = classes.Individual(max_layers=self.max_layers)\n",
    "                    new_arch = getattr(new_individual, 'architecture', None)\n",
    "                    if new_arch is None:\n",
    "                        new_arch = str(new_individual.parsed_layers)\n",
    "\n",
    "                    if new_arch not in unique_architectures:\n",
    "                        unique_architectures.add(new_arch)\n",
    "                        updated_population.append(new_individual)\n",
    "                        break\n",
    "                else:\n",
    "                    # After 50 attempts, keep the original duplicate as a fallback.\n",
    "                    updated_population.append(individual)\n",
    "        return updated_population\n",
    "        \n",
    "    \n",
    "    def build_model(self, parsed_layers):\n",
    "        \"\"\"\n",
    "        Build a model based on the provided parsed layers.\n",
    "\n",
    "        This function creates an encoder using the parsed layers and constructs a model by combining\n",
    "        the encoder with a head layer via the ModelConstructor. The constructed model is built to\n",
    "        process inputs defined by the data module (dm).\n",
    "\n",
    "        Parameters:\n",
    "            parsed_layers: The parsed architecture configuration used by the encoder to build the network.\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch model constructed with the encoder and head layer.\n",
    "        \"\"\"\n",
    "        encoder = classes.generic_network.GenericNetwork(\n",
    "                parsed_layers, \n",
    "                input_channels=self.dm.input_shape[0], \n",
    "                input_height=self.dm.input_shape[1], \n",
    "                input_width=self.dm.input_shape[2], \n",
    "                num_classes=self.dm.num_classes,\n",
    "        )\n",
    "        constructed_model = ModelConstructor(encoder, dm).model\n",
    "        valid = ModelConstructor(encoder, dm).valid_model\n",
    "        return constructed_model, valid\n",
    "    \n",
    "    \n",
    "    def evaluate_parameters(self, model):\n",
    "        \"\"\"\n",
    "        Calculate the total number of parameters of the given model.\n",
    "\n",
    "        Parameters:\n",
    "            model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of parameters.\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        return num_params\n",
    "    \n",
    "    \n",
    "    def _update_df(self):\n",
    "        \"\"\"\n",
    "        Create a DataFrame from the population.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the population.\n",
    "        \"\"\"\n",
    "        columns = [\"Generation\", \"Layers\", \"Fitness\", \"Metric\", \"FPS\", \"Params\"]\n",
    "        data = []\n",
    "        for individual in self.population:\n",
    "            generation = self.generation\n",
    "            parsed_layers = individual.parsed_layers\n",
    "            fitness = individual.fitness\n",
    "            iou = individual.iou\n",
    "            fps = individual.fps\n",
    "            model_size = individual.model_size\n",
    "            data.append([generation, parsed_layers, fitness, iou, fps, model_size])\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns).sort_values(by=\"Fitness\", ascending=False)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        self.df = df\n",
    "    \n",
    "    \n",
    "    def save_dataframe(self):\n",
    "        \"\"\"\n",
    "        Save the DataFrame containing the population statistics to a pickle file.\n",
    "\n",
    "        The DataFrame is saved at a path that includes the current generation number.\n",
    "        In case of an error during saving, the exception details are printed.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        path = f'{self.save_directory}/src/df_population_{self.generation}.pkl'\n",
    "        try:\n",
    "            self.df.to_pickle(path)\n",
    "            print(f\"DataFrame saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving DataFrame to {path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_dataframe(self, generation):\n",
    "        path = f'./models_traced/src/df_population_{generation}.pkl'\n",
    "        try:\n",
    "            df = pd.read_pickle(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DataFrame from {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def save_population(self):\n",
    "        path = f'./models_traced/src/population_{self.generation}.pkl'\n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(self.population, f)\n",
    "            print(f\"Population saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving population to {path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_population(self, generation):\n",
    "        path = f'./models_traced/src/population_{generation}.pkl'\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                population = pickle.load(f)\n",
    "            return population\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading population from {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.population)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Architecture is valid, total parameters: 626 ***\n",
      "Skipping architecture, total parameters: 434820168 exceed the threshold of 100000000\n",
      "Error encountered when checking individual: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 29730 ***\n",
      "*** Architecture is valid, total parameters: 968 ***\n",
      "*** Architecture is valid, total parameters: 245660 ***\n",
      "Error encountered when checking individual: Model size is too big: 246623 Parameters\n",
      "*** Architecture is valid, total parameters: 6206 ***\n",
      "*** Architecture is valid, total parameters: 258510 ***\n",
      "Error encountered when checking individual: Model size is too big: 259089 Parameters\n",
      "*** Architecture is valid, total parameters: 7470 ***\n",
      "*** Architecture is valid, total parameters: 6448 ***\n",
      "*** Architecture is valid, total parameters: 296030 ***\n",
      "Error encountered when checking individual: Model size is too big: 297233 Parameters\n",
      "*** Architecture is valid, total parameters: 9306 ***\n",
      "Skipping architecture, total parameters: 730205860 exceed the threshold of 100000000\n",
      "Error encountered when checking individual: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 31062 ***\n",
      "Skipping architecture, total parameters: 130704080 exceed the threshold of 100000000\n",
      "Error encountered when checking individual: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 2931050 ***\n",
      "Error encountered when checking individual: Model size is too big: 2935013 Parameters\n",
      "*** Architecture is valid, total parameters: 62367002 ***\n",
      "Error encountered when checking individual: Model size is too big: 62382557 Parameters\n",
      "*** Architecture is valid, total parameters: 1669400 ***\n",
      "Error encountered when checking individual: Model size is too big: 1672859 Parameters\n",
      "*** Architecture is valid, total parameters: 14190 ***\n",
      "*** Architecture is valid, total parameters: 6895312 ***\n",
      "Error encountered when checking individual: Model size is too big: 6900499 Parameters\n",
      "*** Architecture is valid, total parameters: 981198 ***\n",
      "Error encountered when checking individual: Model size is too big: 983841 Parameters\n",
      "*** Architecture is valid, total parameters: 6481636 ***\n",
      "Error encountered when checking individual: Model size is too big: 6483943 Parameters\n",
      "*** Architecture is valid, total parameters: 542544 ***\n",
      "Error encountered when checking individual: Model size is too big: 544395 Parameters\n",
      "Skipping architecture, total parameters: 146656310 exceed the threshold of 100000000\n",
      "Error encountered when checking individual: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 2265390 ***\n",
      "Error encountered when checking individual: Model size is too big: 2270577 Parameters\n",
      "*** Architecture is valid, total parameters: 676 ***\n",
      "*** Architecture is valid, total parameters: 544 ***\n",
      "*** Architecture is valid, total parameters: 140388 ***\n",
      "Error encountered when checking individual: Model size is too big: 140787 Parameters\n",
      "*** Architecture is valid, total parameters: 89562292 ***\n",
      "Error encountered when checking individual: Model size is too big: 89592535 Parameters\n",
      "*** Architecture is valid, total parameters: 55139748 ***\n",
      "Error encountered when checking individual: Model size is too big: 55147671 Parameters\n",
      "*** Architecture is valid, total parameters: 35880 ***\n",
      "*** Architecture is valid, total parameters: 959374 ***\n",
      "Error encountered when checking individual: Model size is too big: 962347 Parameters\n",
      "*** Architecture is valid, total parameters: 18976 ***\n",
      "Skipping architecture, total parameters: 122091676 exceed the threshold of 100000000\n",
      "Error encountered when checking individual: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 530 ***\n",
      "*** Architecture is valid, total parameters: 1031636 ***\n",
      "Error encountered when checking individual: Model size is too big: 1034663 Parameters\n",
      "*** Architecture is valid, total parameters: 2243 ***\n",
      "DataFrame saved to ./models_traced/src/df_population_0.pkl\n",
      "Population saved to ./models_traced/src/population_0.pkl\n"
     ]
    }
   ],
   "source": [
    "max_layers = 7\n",
    "pop = Population(15, max_layers, dm=dm)\n",
    "pop.initial_poll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Fitness</th>\n",
       "      <th>Metric</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.2}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.13}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.942202</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 6, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.833195</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.28}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.755361</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.683264</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 12, 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.14}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.609997</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>31533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.563288</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.15}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.425156</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.19}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 11, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.241025</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.38}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.230894</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 11, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.17}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.207942</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.182236</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>36123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.3}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.48}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.173365</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.015966</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Generation  \\\n",
       "0            0   \n",
       "1            0   \n",
       "2            0   \n",
       "3            0   \n",
       "4            0   \n",
       "5            0   \n",
       "6            0   \n",
       "7            0   \n",
       "8            0   \n",
       "9            0   \n",
       "10           0   \n",
       "11           0   \n",
       "12           0   \n",
       "13           0   \n",
       "14           0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Layers  \\\n",
       "0                                                                                                                                                                                                                                                                                       [{'layer_type': 'Dropout', 'dropout_rate': 0.2}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.13}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "1                                                                                                                                                                                                                                 [{'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 6, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "2                                                                                                                                                   [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.28}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "3   [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                     [{'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 12, 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.14}, {'layer_type': 'MaxPool'}]   \n",
       "5                                                                                                                   [{'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "6                                                                                                                                                                                                                                                                                                                                                             [{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.15}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "7                                                                                                                                                                                                                                                                                                        [{'layer_type': 'Dropout', 'dropout_rate': 0.19}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "8                                                                                                                                                                                                                                                                          [{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 11, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                            [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "10                                                                                                                                                                                                                                                                                                                                                           [{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.38}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "11      [{'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 11, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.17}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "12                                                                                                              [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "13                                                                                                                                                                                                                            [{'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.3}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.48}, {'layer_type': 'AvgPool'}]   \n",
       "14                                                                                                                                                                                                                                                                                                                             [{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "\n",
       "     Fitness Metric   FPS  Params  \n",
       "0   0.942202   None  None     635  \n",
       "1   0.833195   None  None   14517  \n",
       "2   0.755361   None  None   19075  \n",
       "3   0.683264   None  None    9459  \n",
       "4   0.609997   None  None   31533  \n",
       "5   0.563288   None  None   30003  \n",
       "6   0.425156   None  None     539  \n",
       "7   0.391061   None  None     571  \n",
       "8   0.385417   None  None    1037  \n",
       "9   0.241025   None  None    6499  \n",
       "10  0.230894   None  None    7713  \n",
       "11  0.207942   None  None    2312  \n",
       "12  0.182236   None  None   36123  \n",
       "13  0.173365   None  None     751  \n",
       "14  0.015966   None  None    6425  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pop.max_layers)\n",
    "\n",
    "for individual in pop:\n",
    "    individual.fitness = np.random.rand() # simulate training\n",
    "pop._update_df()\n",
    "pop.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Architecture is valid, total parameters: 3562730 ***\n",
      "Error encountered when checking individual: Model size is too big: 3566033 Parameters\n",
      "*** Architecture is valid, total parameters: 158164 ***\n",
      "Error encountered when checking individual: Model size is too big: 158599 Parameters\n",
      "*** Architecture is valid, total parameters: 10440 ***\n",
      "*** Architecture is valid, total parameters: 628 ***\n",
      "Error selecting parents: Parents are the same individual.\n",
      "*** Architecture is valid, total parameters: 1810394 ***\n",
      "Error encountered when checking individual: Model size is too big: 1813277 Parameters\n",
      "*** Architecture is valid, total parameters: 7281 ***\n",
      "*** Architecture is valid, total parameters: 22524 ***\n",
      "*** Architecture is valid, total parameters: 1698782 ***\n",
      "Error encountered when checking individual: Model size is too big: 1701665 Parameters\n",
      "*** Architecture is valid, total parameters: 1199456 ***\n",
      "Error encountered when checking individual: Model size is too big: 1201379 Parameters\n",
      "*** Architecture is valid, total parameters: 89212 ***\n",
      "Error selecting parents: Parents are the same individual.\n",
      "*** Architecture is valid, total parameters: 6342 ***\n",
      "*** Architecture is valid, total parameters: 11600 ***\n",
      "*** Architecture is valid, total parameters: 7078 ***\n",
      "*** Architecture is valid, total parameters: 3694 ***\n",
      "*** Architecture is valid, total parameters: 51632 ***\n",
      "*** Architecture is valid, total parameters: 430 ***\n",
      "*** Architecture is valid, total parameters: 1903224 ***\n",
      "Error encountered when evolving population: Model size is 1906107\n",
      "*** Architecture is valid, total parameters: 15860 ***\n",
      "*** Architecture is valid, total parameters: 41970 ***\n",
      "*** Architecture is valid, total parameters: 40924025 ***\n",
      "Error encountered when evolving population: Model size is 40934948\n",
      "Skipping architecture, total parameters: 689691464 exceed the threshold of 100000000\n",
      "Error encountered when evolving population: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 116368 ***\n",
      "Error encountered when evolving population: Model size is 116659\n",
      "*** Architecture is valid, total parameters: 533559 ***\n",
      "Error encountered when evolving population: Model size is 535002\n",
      "*** Architecture is valid, total parameters: 113266 ***\n",
      "Error encountered when evolving population: Model size is 113599\n",
      "Skipping architecture, total parameters: 177057444 exceed the threshold of 100000000\n",
      "Error encountered when evolving population: Encoder appears to have no parameters.\n",
      "Skipping architecture, total parameters: 1046103904 exceed the threshold of 100000000\n",
      "Error encountered when evolving population: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 18852240 ***\n",
      "Error encountered when evolving population: Model size is 18865203\n",
      "*** Architecture is valid, total parameters: 1257920 ***\n",
      "Error encountered when evolving population: Model size is 1260863\n",
      "Skipping architecture, total parameters: 126697680 exceed the threshold of 100000000\n",
      "Error encountered when evolving population: Encoder appears to have no parameters.\n",
      "Skipping architecture, total parameters: 708660482 exceed the threshold of 100000000\n",
      "Error encountered when evolving population: Encoder appears to have no parameters.\n",
      "*** Architecture is valid, total parameters: 1518589 ***\n",
      "Error encountered when evolving population: Model size is 1522288\n",
      "*** Architecture is valid, total parameters: 521304 ***\n",
      "Error encountered when evolving population: Model size is 523155\n",
      "*** Architecture is valid, total parameters: 23298 ***\n",
      "DataFrame saved to ./models_traced/src/df_population_1.pkl\n",
      "Population saved to ./models_traced/src/population_1.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Fitness</th>\n",
       "      <th>Metric</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.2}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.13}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.942202</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.18}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.17}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 6, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>22767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 7, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.15}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>89503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.28}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '3', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>51851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.29}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.28}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.18}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>16103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>42261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.12}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Generation  \\\n",
       "0            1   \n",
       "1            1   \n",
       "2            1   \n",
       "3            1   \n",
       "4            1   \n",
       "5            1   \n",
       "6            1   \n",
       "7            1   \n",
       "8            1   \n",
       "9            1   \n",
       "10           1   \n",
       "11           1   \n",
       "12           1   \n",
       "13           1   \n",
       "14           1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Layers  \\\n",
       "0                                                                                                                                                                         [{'layer_type': 'Dropout', 'dropout_rate': 0.2}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.13}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "1                                                                                                                                                                                            [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.18}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "2                                                                                                                                                                                       [{'layer_type': 'Dropout', 'dropout_rate': 0.17}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "3                                                                                                                                                      [{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 6, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "4     [{'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "5                                                                                                                      [{'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 7, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.15}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "6                                                                                                                                                                                           [{'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.28}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "7                                                                                                                                                                   [{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "8                                                           [{'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "9                                                       [{'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "10                                                                                                                                                            [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '3', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "11                                                                                                                                                                                                                                                                                [{'layer_type': 'Dropout', 'dropout_rate': 0.29}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.28}, {'layer_type': 'MaxPool'}]   \n",
       "12                                                                                                                                                                                                                                           [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.18}, {'layer_type': 'MaxPool'}]   \n",
       "13  [{'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "14                                                                                                                                   [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 9, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.12}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "\n",
       "     Fitness Metric   FPS  Params  \n",
       "0   0.942202   None  None     635  \n",
       "1   0.000000   None  None   10503  \n",
       "2   0.000000   None  None     703  \n",
       "3   0.000000   None  None    7500  \n",
       "4   0.000000   None  None   22767  \n",
       "5   0.000000   None  None   89503  \n",
       "6   0.000000   None  None    6399  \n",
       "7   0.000000   None  None   11657  \n",
       "8   0.000000   None  None    7153  \n",
       "9   0.000000   None  None    3745  \n",
       "10  0.000000   None  None   51851  \n",
       "11  0.000000   None  None     439  \n",
       "12  0.000000   None  None   16103  \n",
       "13  0.000000   None  None   42261  \n",
       "14  0.000000   None  None   23679  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.evolve(mating_pool_cutoff=0.5, mutation_probability=0.85, k_best=1, n_random=3)\n",
    "pop.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9422017556848528"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.topModels[0].fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model fresh created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASTrainer:\n",
    "    def __init__(self, population, idx, dm, lr, max_epochs=10):\n",
    "        self.population = population\n",
    "        self.idx = idx\n",
    "        self.dm = dm\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        # Build the model from the selected individual.\n",
    "        layers = self.population[self.idx].parsed_layers\n",
    "        self.constructed_model, is_valid = self.population.build_model(layers)\n",
    "        if not is_valid:\n",
    "            raise ValueError(\"Constructed model is not valid.\")\n",
    "        \n",
    "        self.LM = classes.GenericLightningNetwork(\n",
    "            model=self.constructed_model,\n",
    "            num_classes=self.dm.num_classes,\n",
    "            learning_rate=self.lr,\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        # Train the lightning model\n",
    "        self.trainer.fit(self.LM, self.dm)\n",
    "        self.results = self.trainer.test(self.LM, self.dm)\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_model(self, save_torchscript=True, \n",
    "                   ts_save_path=None,\n",
    "                   save_standard=True, \n",
    "                   std_save_path=None):\n",
    "        # Use generation attribute from the Population object.\n",
    "        gen = self.population.generation\n",
    "        \n",
    "        if ts_save_path is None:\n",
    "            ts_save_path = f\"models_traced/generation_{gen}/model_and_architecture_{self.idx}.pt\"\n",
    "        if std_save_path is None:\n",
    "            std_save_path = f\"models_traced/generation_{gen}/model_{self.idx}.pth\"\n",
    "        \n",
    "        # Save the results to a text file.\n",
    "        with open(f\"models_traced/generation_{gen}/results_model_{self.idx}.txt\", \"w\") as f:\n",
    "            f.write(\"Test Results:\\n\")\n",
    "            for key, value in self.results[0].items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        # Prepare dummy input from dm.input_shape\n",
    "        input_shape = self.dm.input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            input_shape = (1,) + input_shape\n",
    "        device = next(self.LM.parameters()).device\n",
    "        example_input = torch.randn(*input_shape).to(device)\n",
    "        \n",
    "        self.LM.eval()  # set the model to evaluation mode\n",
    "        \n",
    "        if save_torchscript:\n",
    "            traced_model = torch.jit.trace(self.LM.model, example_input)\n",
    "            traced_model.save(ts_save_path)\n",
    "            print(f\"Scripted (TorchScript) model saved at {ts_save_path}\")\n",
    "        \n",
    "        if save_standard:\n",
    "            # Retrieve architecture code from the individual.\n",
    "            arch_code = self.population[self.idx].architecture\n",
    "            save_dict = {\"state_dict\": self.LM.model.state_dict()}\n",
    "            if arch_code is not None:\n",
    "                save_dict[\"architecture_code\"] = arch_code\n",
    "            torch.save(save_dict, std_save_path)\n",
    "            print(f\"Standard model saved at {std_save_path}\")\n",
    "\n",
    "\n",
    "from myFit import FitnessEvaluator\n",
    "evaluator = FitnessEvaluator()\n",
    "\n",
    "# Train the models in the population           \n",
    "for idx in range(len(pop)): \n",
    "    nt = NASTrainer(population=pop, idx=idx, dm=dm, lr=1e-3, max_epochs=2)\n",
    "    nt.train()\n",
    "    nt.save_model()\n",
    "    \n",
    "    \n",
    "    # API to update the population with the results from the model training\n",
    "    result = # caller_api(nt.model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update the population with the results from the model training\n",
    "    fps = nt.results[0]['fps']\n",
    "    metric = nt.results[0]['test_mcc']\n",
    "    ####\n",
    "    pop[idx].iou = nt.results[0]['test_mcc']\n",
    "    pop[idx].fps = nt.results[0]['fps']\n",
    "    \n",
    "    pop[idx].fitness = evaluator.weighted_sum_exponential(fps, metric)\n",
    "    \n",
    "    pop.df.loc[idx, 'Fitness'] = pop[idx].fitness\n",
    "    pop.df.loc[idx, 'Metric'] = pop[idx].iou\n",
    "    pop.df.loc[idx, 'FPS'] = pop[idx].fps\n",
    "    \n",
    "    pop.save_dataframe()\n",
    "    pop.save_population()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making new generation, some important things:\n",
    "\n",
    "- check model size below the thresh when creating new child, otherwise go for another tentative.\n",
    "- retain best K model at each generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "# new_pop = generate_new_population(pop, mating_pool_cutoff, mutation_probability, k_best=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_population[2].parsed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[2].parsed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" * 20)\n",
    "print(f\"*** GENERATION {t} ***\")\n",
    "new_population = []\n",
    "\n",
    "# Create a mating pool\n",
    "mating_pool = population[:int(np.floor(mating_pool_cutoff * len(population)))].copy()\n",
    "for i in range(int(np.ceil((1 - mating_pool_cutoff) * len(population)))):\n",
    "    temp_individual = classes.Individual(max_layers=max_layers)\n",
    "    mating_pool.append(temp_individual)\n",
    "\n",
    "# Coupling and mating\n",
    "couple_i = 0\n",
    "while couple_i < len(mating_pool):\n",
    "    parents = [mating_pool[couple_i], mating_pool[couple_i + 1]]\n",
    "    children = single_point_crossover(parents=parents)\n",
    "    children = mutation(children=children, mutation_probability=mutation_probability )\n",
    "    new_population = new_population + children\n",
    "    couple_i += 2\n",
    "\n",
    "# Update the population\n",
    "population = new_population.copy()\n",
    "for i in population:\n",
    "    i.architecture = i.chromosome2architecture(i.chromosome)\n",
    "population = remove_duplicates(population=population, max_layers=max_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved TorchScript model and test with a dummy input.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_path = \"model_and_architecture.pt\"\n",
    "loaded_model = torch.jit.load(save_path, map_location=device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Ensure input is moved to the correct device\n",
    "example_input = torch.randn(1, *dm.input_shape).to(device)\n",
    "example_input = example_input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(example_input)\n",
    "print(\"Output from the loaded model:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
