{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model training with a generic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "from datasets.RawClassifier.loader import RawClassifierDataModule, RawClassifierDataset\n",
    "\n",
    "# Define dataset module\n",
    "root_dir = '/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/RawClassifier'\n",
    "dm = RawClassifierDataModule(root_dir, batch_size=4, num_workers=2, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch Lightning module\n",
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x.float())\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train_model(encoder, dm, max_epochs=10):\n",
    "    model = LitModule(encoder, dm.num_classes)\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Train the model\n",
    "    trainer.fit(model, dm)\n",
    "    return trainer \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = resnet18(pretrained=True, )\n",
    "\n",
    "# Initialize a resnet18 encoder from torchvision, use pretrained weights\n",
    "encoder = resnet18(pretrained=True)\n",
    "encoder.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False) # INPUT\n",
    "encoder.fc = nn.Identity() # Out only features, no classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the defined train_model function and the existing data module (dm)\n",
    "trainer = train_model(encoder, dm, max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------The batch size of the data to be loaded in the model is: 4-----------\n"
     ]
    }
   ],
   "source": [
    "import pynattas as pnas\n",
    "from pynattas import classes, functions\n",
    "from pynattas.optimizers.ga import single_point_crossover, mutation \n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "from datasets.RawClassifier.loader import RawClassifierDataModule, RawClassifierDataset\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define dataset module\n",
    "root_dir = '/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/RawClassifier'\n",
    "dm = RawClassifierDataModule(root_dir, batch_size=4, num_workers=2, transform=None)\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Model parameters\n",
    "max_layers = int(config.getint('NAS', 'max_layers'))\n",
    "max_iter = int(config['GA']['max_iterations'])\n",
    "# GA parameters\n",
    "n_individuals = int(config['GA']['population_size'])\n",
    "mating_pool_cutoff = float(config['GA']['mating_pool_cutoff'])\n",
    "mutation_probability = float(config['GA']['mutation_probability'])\n",
    "# Logging\n",
    "logs_directory = str(config['GA']['logs_dir_GA'])\n",
    "\n",
    "# Torch stuff\n",
    "seed = config.getint(section='Computation', option='seed')\n",
    "pl.seed_everything(seed=seed, workers=True)  # For reproducibility\n",
    "torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy\n",
    "num_workers = config.getint(section='Computation', option='num_workers')\n",
    "accelerator = config.get(section='Computation', option='accelerator')\n",
    "\n",
    "log_learning_rate=None\n",
    "batch_size=None\n",
    "# Get model parameters\n",
    "log_lr = log_learning_rate if log_learning_rate is not None else config.getfloat(section='Search Space', option='default_log_lr')\n",
    "\n",
    "lr = 10**log_lr\n",
    "bs = batch_size if batch_size is not None else config.getint(section='Search Space', option='default_bs')\n",
    "print(f\"-----------The batch size of the data to be loaded in the model is: {bs}-----------\")\n",
    "def initialize_logging(max_iter):\n",
    "    mean_fitness_vector = np.zeros(shape=(max_iter + 1))\n",
    "    median_fitness_vector = np.zeros_like(mean_fitness_vector)\n",
    "    best_fitness_vector = np.zeros_like(mean_fitness_vector)\n",
    "    iou_vector = np.zeros_like(mean_fitness_vector)\n",
    "    fps_vector = np.zeros_like(mean_fitness_vector)\n",
    "    model_size_vector = np.zeros_like(mean_fitness_vector)\n",
    "\n",
    "    historical_best_fitness = float('-inf')\n",
    "    historical_best_iou = float('-inf')\n",
    "    historical_best_fps = float('-inf')\n",
    "    historical_best_model_size = float('inf')\n",
    "\n",
    "    best_individual = None  # To keep track of the best individual\n",
    "\n",
    "    return {\n",
    "        \"mean_fitness_vector\": mean_fitness_vector,\n",
    "        \"median_fitness_vector\": median_fitness_vector,\n",
    "        \"best_fitness_vector\": best_fitness_vector,\n",
    "        \"iou_vector\": iou_vector,\n",
    "        \"fps_vector\": fps_vector,\n",
    "        \"model_size_vector\": model_size_vector,\n",
    "        \"historical_best_fitness\": historical_best_fitness,\n",
    "        \"historical_best_iou\": historical_best_iou,\n",
    "        \"historical_best_fps\": historical_best_fps,\n",
    "        \"historical_best_model_size\": historical_best_model_size,\n",
    "        \"best_individual\": best_individual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConstructor:\n",
    "    def __init__(self, encoder, dm):\n",
    "        # Validate that dm has the necessary attributes.\n",
    "        if not hasattr(dm, \"num_classes\") or not hasattr(dm, \"input_shape\"):\n",
    "            raise ValueError(\"dm must have 'num_classes' and 'input_shape' attributes.\")\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.num_classes = dm.num_classes\n",
    "        self.input_shape = dm.input_shape\n",
    "        print(f\"Input shape: {self.input_shape}\")\n",
    "        \n",
    "        # Verify that encoder has parameters.\n",
    "        try:\n",
    "            next(self.encoder.parameters())\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Encoder appears to have no parameters.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Provided encoder does not follow expected API.\") from e\n",
    "\n",
    "        # Validate input_shape is a tuple and properly dimensioned.\n",
    "        if not isinstance(self.input_shape, tuple):\n",
    "            raise TypeError(\"input_shape must be a tuple.\")\n",
    "        if len(self.input_shape) == 3:\n",
    "            print(\"Adding channel dimension to input shape.\")\n",
    "            print(f\"Original input shape: {self.input_shape}\")\n",
    "            self.input_shape = (1,) + self.input_shape\n",
    "            print(f\"Updated input shape: {self.input_shape}\")\n",
    "        elif len(self.input_shape) != 4:\n",
    "            raise ValueError(\"input_shape must be of length 3 or 4.\")\n",
    "\n",
    "        self.head_layer = self.build_head(input_shape=self.input_shape)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.encoder,\n",
    "            self.head_layer\n",
    "        )\n",
    "        \n",
    "        self.valid_model = self.dummy_test()\n",
    "\n",
    "    def build_head(self, input_shape=(1, 2, 256, 256)):\n",
    "        # Get the device from the encoder's parameters.\n",
    "        try:\n",
    "            device = next(self.encoder.parameters()).device\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Unable to determine device from encoder parameters.\") from e\n",
    "        \n",
    "        # Run a dummy input through the encoder to get the feature shape.\n",
    "        dummy = torch.randn(*input_shape).float().to(device)\n",
    "        try:\n",
    "            features = self.encoder(dummy)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Error when running dummy input through encoder.\") from e\n",
    "        \n",
    "        if not isinstance(features, torch.Tensor):\n",
    "            raise TypeError(\"Encoder output should be a torch.Tensor.\")\n",
    "\n",
    "        print(\"Feature map shape from the feature extractor:\", features.shape)\n",
    "\n",
    "        # Check that the features tensor has at least 2 dimensions.\n",
    "        if features.dim() < 2:\n",
    "            raise ValueError(\"Encoded features should have at least 2 dimensions.\")\n",
    "        \n",
    "        # Determine the number of channels from the dummy output.\n",
    "        feature_channels = features.shape[1]\n",
    "\n",
    "        # Build the head layer.\n",
    "        head_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_channels, self.num_classes)\n",
    "        )\n",
    "        print(\"Constructed head layer:\", head_layer)\n",
    "        return head_layer\n",
    "    \n",
    "    \n",
    "    def dummy_test(self):\n",
    "        try:\n",
    "            device = next(self.encoder.parameters()).device\n",
    "            dummy = torch.randn(*self.input_shape).float().to(device)\n",
    "            output = self.model(dummy)\n",
    "            print(\"Network test passed. Output shape from the model:\", output.shape)\n",
    "            \n",
    "            if not isinstance(output, torch.Tensor):\n",
    "                raise TypeError(\"Output of the model should be a torch.Tensor.\")\n",
    "            \n",
    "            if output.shape[0] != dummy.shape[0]:\n",
    "                raise ValueError(\"Batch size mismatch between input and output.\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during dummy_test:\", e)\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor.\")\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutation import gene_mutation\n",
    "from crossover import single_point_crossover\n",
    "\n",
    "class Population:\n",
    "    def __init__(self, n_individuals, max_layers, dm, max_parameters=100000):\n",
    "        self.dm = dm # Data module for model creation\n",
    "        \n",
    "        self.n_individuals = n_individuals\n",
    "        self.max_layers = max_layers\n",
    "        self.generation = 0\n",
    "        self.max_parameters = max_parameters\n",
    "        \n",
    "        \n",
    "    def initial_poll(self):\n",
    "        \"\"\"\n",
    "        Generate the initial population of individuals.    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.population = self.create_population()\n",
    "        self._update_df()\n",
    "        self.save_dataframe()\n",
    "        self.save_population()\n",
    "\n",
    "\n",
    "    def create_random_individual(self):\n",
    "        \"\"\"\n",
    "        Create a random individual with a random number of layers.\n",
    "        \"\"\"\n",
    "        return classes.Individual(max_layers=self.max_layers)\n",
    "    \n",
    "\n",
    "    def sort_population(self):\n",
    "        \"\"\"\n",
    "        Sort the population by fitness.\n",
    "        \"\"\"\n",
    "        self.population = sorted(self.population, key=lambda individual: individual.fitness, reverse=True)\n",
    "        self.checkpoint()\n",
    "        \n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"\n",
    "        Save the current population.\n",
    "        \"\"\"\n",
    "        self._update_df()\n",
    "        self.save_population()\n",
    "        self.save_dataframe()\n",
    "    \n",
    "    \n",
    "    def check_individual(self, individual):\n",
    "        try:\n",
    "            model_representation, is_valid = self.build_model(individual.parsed_layers)\n",
    "            if is_valid:\n",
    "                modelSize = self.evaluate_parameters(model_representation)\n",
    "                individual.model_size = modelSize\n",
    "                \n",
    "                assert modelSize > 0, f\"Model size must be greater then zero: {modelSize} Parameters\"\n",
    "                assert modelSize < self.max_parameters, f\"Model size is too big: {modelSize} Parameters\"\n",
    "                assert modelSize is not None, f\"Model size is None...\"\n",
    "                return True # Individual is valid\n",
    "        except Exception as e:\n",
    "                print(f\"Error encountered when checking individual: {e}\")\n",
    "                return False # Individual is invalid\n",
    "\n",
    "\n",
    "\n",
    "    def create_population(self):\n",
    "        population = []\n",
    "        # Generate individuals until the population reaches n_individuals, removing duplicates along the way\n",
    "        while len(population) < self.n_individuals:\n",
    "            candidate = self.create_random_individual() # Create a random individual\n",
    "            if self.check_individual(candidate):\n",
    "                population.append(candidate)\n",
    "            \n",
    "            population = self.remove_duplicates(population) # Remove duplicates\n",
    "        return population\n",
    "\n",
    "\n",
    "    def elite_models(self, k_best=1):\n",
    "        \"\"\"\n",
    "        Get the k_best models from the current population.\n",
    "        \"\"\"\n",
    "        sorted_pop = sorted(self, key=lambda individual: individual.fitness, reverse=True)\n",
    "        topModels = [sorted_pop[i].copy() for i in range(k_best)]\n",
    "        return topModels\n",
    "\n",
    "\n",
    "    def evolve(self, mating_pool_cutoff=0.5, mutation_probability=0.85, k_best=1, n_random=3):\n",
    "        \"\"\"\n",
    "        Generates a new population ensuring that the total number of individuals equals pop.n_individuals.\n",
    "        \n",
    "        Parameters:\n",
    "            pop                  : List or collection of individuals. Assumed to have attributes: \n",
    "                                .n_individuals and .generation.\n",
    "            mating_pool_cutoff   : Fraction determining the size of the mating pool (top percent of individuals).\n",
    "            mutation_probability : The probability to use during mutation.\n",
    "            k_best               : The number of best individuals from the current population to retain.\n",
    "        \n",
    "        Returns:\n",
    "            new_population: A list representing the new generation of individuals.\n",
    "            \n",
    "        Note:\n",
    "            Assumes that helper functions single_point_crossover(), mutation(), and create_random_individual() exist.\n",
    "        \"\"\"\n",
    "        new_population = []\n",
    "        self.generation += 1\n",
    "        self.topModels = self.elite_models(k_best=k_best)\n",
    "\n",
    "\n",
    "        # 2. Create the mating pool based on the cutoff from the sorted population\n",
    "        sorted_pop = sorted(self, key=lambda individual: individual.fitness, reverse=True)\n",
    "        mating_pool = sorted_pop[:int(np.floor(mating_pool_cutoff * self.n_individuals))].copy()\n",
    "        assert len(mating_pool) > 0, \"Mating pool is empty.\"\n",
    "        \n",
    "        # Generate offspring until reaching the desired population size\n",
    "        while len(new_population) < self.n_individuals - n_random - k_best:\n",
    "            try:\n",
    "                parent1 = np.random.choice(mating_pool)\n",
    "                parent2 = np.random.choice(mating_pool)\n",
    "                assert parent1.parsed_layers != parent2.parsed_layers, \"Parents are the same individual.\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting parents: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # a) Crossover:\n",
    "            child = single_point_crossover([parent1, parent2])\n",
    "            # b) Mutation:\n",
    "            mutated_child = gene_mutation(child, mutation_probability)\n",
    "            # c) Random choice of one of the mutated children\n",
    "            for kid in mutated_child:\n",
    "                kid.reset()\n",
    "                if self.check_individual(kid):\n",
    "                    new_population.append(kid)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        # 3. Add random individuals to the new population\n",
    "        while len(new_population) < self.n_individuals - k_best:\n",
    "            try:\n",
    "                individual = self.create_random_individual()\n",
    "                model_representation, is_valid = self.build_model(individual.parsed_layers)\n",
    "                if is_valid:\n",
    "                    individual.model_size = int(self.evaluate_parameters(model_representation))\n",
    "                    assert individual.model_size > 0, f\"Model size is {individual.model_size}\"\n",
    "                    assert individual.model_size < self.max_parameters, f\"Model size is {individual.model_size}\"\n",
    "                    assert individual.model_size is not None, f\"Model size is None\"\n",
    "                    new_population.append(individual)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered when evolving population: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # 4. Add the best individuals from the previous generation\n",
    "        new_population.extend(self.topModels)\n",
    "       \n",
    "\n",
    "        assert len(new_population) == self.n_individuals, f\"Population size is {len(new_population)}, expected {self.n_individuals}\"\n",
    "        self.population = new_population\n",
    "        self._update_df()\n",
    "        self.save_dataframe()\n",
    "        self.save_population()\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.population[index]\n",
    "\n",
    "\n",
    "    def remove_duplicates(self, population):\n",
    "        \"\"\"\n",
    "        Remove duplicates from the given population by replacing duplicates with newly generated unique individuals.\n",
    "\n",
    "        Parameters:\n",
    "            population (list): A list of individuals in the population.\n",
    "\n",
    "        Returns:\n",
    "            list: The updated population with duplicates removed.\n",
    "        \"\"\"\n",
    "        unique_architectures = set()\n",
    "        updated_population = []\n",
    "\n",
    "        for individual in population:\n",
    "            # Use the 'architecture' attribute if available, otherwise fallback to a default representation.\n",
    "            arch = getattr(individual, 'architecture', None)\n",
    "            if arch is None:\n",
    "                # If no architecture attribute, use parsed_layers as unique identifier.\n",
    "                arch = str(individual.parsed_layers)\n",
    "\n",
    "            if arch not in unique_architectures:\n",
    "                unique_architectures.add(arch)\n",
    "                updated_population.append(individual)\n",
    "            else:\n",
    "                # Try to generate a unique individual up to 50 times\n",
    "                for _ in range(50):\n",
    "                    new_individual = classes.Individual(max_layers=self.max_layers)\n",
    "                    new_arch = getattr(new_individual, 'architecture', None)\n",
    "                    if new_arch is None:\n",
    "                        new_arch = str(new_individual.parsed_layers)\n",
    "\n",
    "                    if new_arch not in unique_architectures:\n",
    "                        unique_architectures.add(new_arch)\n",
    "                        updated_population.append(new_individual)\n",
    "                        break\n",
    "                else:\n",
    "                    # After 50 attempts, keep the original duplicate as a fallback.\n",
    "                    updated_population.append(individual)\n",
    "        return updated_population\n",
    "        \n",
    "    \n",
    "    def build_model(self, parsed_layers):\n",
    "        \"\"\"\n",
    "        Build a model based on the provided parsed layers.\n",
    "\n",
    "        This function creates an encoder using the parsed layers and constructs a model by combining\n",
    "        the encoder with a head layer via the ModelConstructor. The constructed model is built to\n",
    "        process inputs defined by the data module (dm).\n",
    "\n",
    "        Parameters:\n",
    "            parsed_layers: The parsed architecture configuration used by the encoder to build the network.\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch model constructed with the encoder and head layer.\n",
    "        \"\"\"\n",
    "        encoder = classes.generic_network.GenericNetwork(\n",
    "                parsed_layers, \n",
    "                input_channels=self.dm.input_shape[0], \n",
    "                input_height=self.dm.input_shape[1], \n",
    "                input_width=self.dm.input_shape[2], \n",
    "                num_classes=self.dm.num_classes,\n",
    "        )\n",
    "        constructed_model = ModelConstructor(encoder, dm).model\n",
    "        valid = ModelConstructor(encoder, dm).valid_model\n",
    "        return constructed_model, valid\n",
    "    \n",
    "    \n",
    "    def evaluate_parameters(self, model):\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        return num_params\n",
    "    \n",
    "    \n",
    "    def _update_df(self):\n",
    "            \"\"\"\n",
    "            Create a DataFrame from the population.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: A DataFrame containing the population.\n",
    "            \"\"\"\n",
    "            columns = [\"Generation\", \"Layers\", \"Fitness\", \"Metric\", \"FPS\", \"Params\"]\n",
    "            data = []\n",
    "            for individual in self.population:\n",
    "                generation = self.generation\n",
    "                parsed_layers = individual.parsed_layers\n",
    "                fitness = individual.fitness\n",
    "                iou = individual.iou\n",
    "                fps = individual.fps\n",
    "                model_size = individual.model_size\n",
    "                data.append([generation, parsed_layers, fitness, iou, fps, model_size])\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns).sort_values(by=\"Fitness\", ascending=False)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            self.df = df\n",
    "    \n",
    "    \n",
    "    def save_dataframe(self):\n",
    "        path = f'./models_traced/src/df_population_{self.generation}.pkl'\n",
    "        try:\n",
    "            self.df.to_pickle(path)\n",
    "            print(f\"DataFrame saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving DataFrame to {path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_dataframe(self, generation):\n",
    "        path = f'./models_traced/src/df_population_{generation}.pkl'\n",
    "        try:\n",
    "            df = pd.read_pickle(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DataFrame from {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def save_population(self):\n",
    "        path = f'./models_traced/src/population_{self.generation}.pkl'\n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(self.population, f)\n",
    "            print(f\"Population saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving population to {path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_population(self, generation):\n",
    "        path = f'./models_traced/src/population_{generation}.pkl'\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                population = pickle.load(f)\n",
    "            return population\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading population from {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.population)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture is valid, total parameters: 18732\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 112, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=112, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 112, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=112, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 3303\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 12, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=12, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 12, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=12, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 430\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 2, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=2, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 2, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=2, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 2157990\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 1188, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=1188, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 1188, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=1188, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Error encountered when checking individual: Model size is too big: 2161557 Parameters\n",
      "Architecture is valid, total parameters: 1490\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 20, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 20, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 76410936\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 6160, 32, 32])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=6160, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 6160, 32, 32])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=6160, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Error encountered when checking individual: Model size is too big: 76429419 Parameters\n",
      "Architecture is valid, total parameters: 5954\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 22, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=22, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 22, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=22, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 21856\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 88, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=88, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 88, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=88, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 82672580\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 8064, 32, 32])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=8064, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 8064, 32, 32])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=8064, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "max_layers = 5\n",
    "pop = Population(15, max_layers, dm=dm)\n",
    "pop.initial_poll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "8\n",
      "8\n",
      "6\n",
      "6\n",
      "6\n",
      "8\n",
      "6\n",
      "6\n",
      "10\n",
      "8\n",
      "6\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Fitness</th>\n",
       "      <th>Metric</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.969910</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 9, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>31743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.832443</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>74859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.708073</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.601115</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>93091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.31}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.374540</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 10, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.19}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.11}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.19}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.2}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 10, 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>22636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.26}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Generation  \\\n",
       "0            0   \n",
       "1            0   \n",
       "2            0   \n",
       "3            0   \n",
       "4            0   \n",
       "5            0   \n",
       "6            0   \n",
       "7            0   \n",
       "8            0   \n",
       "9            0   \n",
       "10           0   \n",
       "11           0   \n",
       "12           0   \n",
       "13           0   \n",
       "14           0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Layers  \\\n",
       "0                          [{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]   \n",
       "1                                                                                                                                                                                                          [{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 9, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "2                                                                                                     [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "3                                                                                                           [{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "4                                                                                                                                      [{'layer_type': 'Dropout', 'dropout_rate': 0.1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "5                                                                                                                                                           [{'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "6                                                  [{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "7   [{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "8                                                                                                                                                                                       [{'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.31}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "9                                                                                                                                                 [{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 10, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "10                                                              [{'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConv', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.19}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.11}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "11                                                                                                                                                                   [{'layer_type': 'Dropout', 'dropout_rate': 0.19}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.2}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "12                                                                                                                                                   [{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 10, 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "13                                                                                                         [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "14                                                                                                                                                                                          [{'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.26}, {'layer_type': 'AvgPool'}]   \n",
       "\n",
       "     Fitness Metric   FPS  Params  \n",
       "0   0.969910   None  None    4943  \n",
       "1   0.950714   None  None    1158  \n",
       "2   0.866176   None  None   31743  \n",
       "3   0.832443   None  None    7057  \n",
       "4   0.731994   None  None   74859  \n",
       "5   0.708073   None  None     511  \n",
       "6   0.601115   None  None    4601  \n",
       "7   0.598658   None  None   93091  \n",
       "8   0.374540   None  None     775  \n",
       "9   0.212339   None  None   37385  \n",
       "10  0.181825   None  None     367  \n",
       "11  0.156019   None  None     133  \n",
       "12  0.155995   None  None   22636  \n",
       "13  0.058084   None  None   23283  \n",
       "14  0.020584   None  None     315  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pop.max_layers)\n",
    "\n",
    "for individual in pop:\n",
    "    individual.fitness = np.random.rand() # simulate training\n",
    "pop._update_df()\n",
    "pop.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture is valid, total parameters: 73992\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 288, 126, 126])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=288, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 288, 126, 126])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=288, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 4760\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 60, 31, 31])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=60, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 60, 31, 31])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=60, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 7000\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 18, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=18, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 18, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=18, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 1095\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 20, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 20, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 460\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 16, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 16, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 73992\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 288, 126, 126])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=288, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 288, 126, 126])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=288, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 7000\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 18, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=18, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 18, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=18, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 4760\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 60, 31, 31])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=60, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 60, 31, 31])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=60, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 4760\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 60, 31, 31])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=60, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 60, 31, 31])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=60, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 31320\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 140, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=140, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 140, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=140, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 31320\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 140, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=140, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 140, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=140, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 4550\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 16, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 16, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 519\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 14, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=14, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 14, 125, 125])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=14, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Architecture is valid, total parameters: 33981\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 132, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=132, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "Input shape: (2, 1000, 1000)\n",
      "Adding channel dimension to input shape.\n",
      "Original input shape: (2, 1000, 1000)\n",
      "Updated input shape: (1, 2, 1000, 1000)\n",
      "Feature map shape from the feature extractor: torch.Size([1, 132, 62, 62])\n",
      "Constructed head layer: Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=132, out_features=3, bias=True)\n",
      ")\n",
      "Network test passed. Output shape from the model: torch.Size([1, 3])\n",
      "DataFrame saved to ./models_traced/src/df_population_1.pkl\n",
      "Population saved to ./models_traced/src/population_1.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Fitness</th>\n",
       "      <th>Metric</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.96991</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>74859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 9, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'Dropout', 'dropout_rate': 0.1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>74859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>31743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>31743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.47}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 6, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 11, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>34380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Generation  \\\n",
       "0            1   \n",
       "1            1   \n",
       "2            1   \n",
       "3            1   \n",
       "4            1   \n",
       "5            1   \n",
       "6            1   \n",
       "7            1   \n",
       "8            1   \n",
       "9            1   \n",
       "10           1   \n",
       "11           1   \n",
       "12           1   \n",
       "13           1   \n",
       "14           1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Layers  \\\n",
       "0   [{'layer_type': 'ConvAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 4, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "1                                                                                                                                                                                                                                                                                                         [{'layer_type': 'Dropout', 'dropout_rate': 0.1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "2                                                                                                                                                                                             [{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]   \n",
       "3                                                                                                                                                                                                                                                                              [{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                             [{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 9, 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "5                                                                                                                                                                                                                                                                                                                              [{'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '2', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}]   \n",
       "6                                                                                                                                                                                                                                                                                                         [{'layer_type': 'Dropout', 'dropout_rate': 0.1}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvSE', 'out_channels_coefficient': 12, 'kernel_size': '3', 'stride': '1', 'padding': '2', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "7                                                                                                                                                                                                                                                                              [{'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 8, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'MBConv', 'expansion_factor': '4', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '3', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}]   \n",
       "8                                                                                                                                                                                             [{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]   \n",
       "9                                                                                                                                                                                             [{'layer_type': 'ConvAct', 'out_channels_coefficient': 5, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'DenseNetBlock', 'out_channels_coefficient': 5, 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.37}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.35}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.46}, {'layer_type': 'MaxPool'}]   \n",
       "10                                                                                                                                                                                                                                                                       [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "11                                                                                                                                                                                                                                                                       [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 10, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'ReLU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "12                                                                                                                                                                                                                    [{'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 8, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'MBConvNoRes', 'expansion_factor': '5', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "13                                                                                                                                                                                                                                                                                                                                                          [{'layer_type': 'ConvBnAct', 'out_channels_coefficient': 7, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'Dropout', 'dropout_rate': 0.47}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}]   \n",
       "14                                                                                                                                                              [{'layer_type': 'MBConvNoRes', 'expansion_factor': '6', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvBnAct', 'out_channels_coefficient': 6, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'MaxPool'}, {'layer_type': 'ConvAct', 'out_channels_coefficient': 11, 'kernel_size': '3', 'stride': '1', 'padding': '1', 'activation': 'GELU'}, {'layer_type': 'AvgPool'}, {'layer_type': 'ResNetBlock', 'reduction_factor': '4', 'activation': 'ReLU'}, {'layer_type': 'MaxPool'}]   \n",
       "\n",
       "    Fitness Metric   FPS  Params  \n",
       "0   0.96991   None  None    4943  \n",
       "1   0.00000   None  None   74859  \n",
       "2   0.00000   None  None    4943  \n",
       "3   0.00000   None  None    7057  \n",
       "4   0.00000   None  None    1158  \n",
       "5   0.00000   None  None     511  \n",
       "6   0.00000   None  None   74859  \n",
       "7   0.00000   None  None    7057  \n",
       "8   0.00000   None  None    4943  \n",
       "9   0.00000   None  None    4943  \n",
       "10  0.00000   None  None   31743  \n",
       "11  0.00000   None  None   31743  \n",
       "12  0.00000   None  None    4601  \n",
       "13  0.00000   None  None     564  \n",
       "14  0.00000   None  None   34380  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.evolve(mating_pool_cutoff=0.5, mutation_probability=0.85, k_best=1, n_random=3)\n",
    "pop.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.topModels[0].fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutation and Crossover to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.evolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.topModels[0].fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for individual in pop:\n",
    "    print(individual.model_size)\n",
    "    \n",
    "m, _ = pop.build_model(pop[2].parsed_layers)\n",
    "\n",
    "\n",
    "pop.evaluate_parameters(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.build_model(pop[0].parsed_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model fresh created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASTrainer:\n",
    "    def __init__(self, population, idx, dm, lr, max_epochs=10):\n",
    "        self.population = population\n",
    "        self.idx = idx\n",
    "        self.dm = dm\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        # Build the model from the selected individual.\n",
    "        layers = self.population[self.idx].parsed_layers\n",
    "        self.constructed_model, is_valid = self.population.build_model(layers)\n",
    "        if not is_valid:\n",
    "            raise ValueError(\"Constructed model is not valid.\")\n",
    "        \n",
    "        self.LM = classes.GenericLightningNetwork(\n",
    "            model=self.constructed_model,\n",
    "            num_classes=self.dm.num_classes,\n",
    "            learning_rate=self.lr,\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        # Train the lightning model\n",
    "        self.trainer.fit(self.LM, self.dm)\n",
    "        self.results = self.trainer.test(self.LM, self.dm)\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_model(self, save_torchscript=True, \n",
    "                   ts_save_path=None,\n",
    "                   save_standard=True, \n",
    "                   std_save_path=None):\n",
    "        # Use generation attribute from the Population object.\n",
    "        gen = self.population.generation\n",
    "        \n",
    "        if ts_save_path is None:\n",
    "            ts_save_path = f\"models_traced/generation_{gen}/model_and_architecture_{self.idx}.pt\"\n",
    "        if std_save_path is None:\n",
    "            std_save_path = f\"models_traced/generation_{gen}/model_{self.idx}.pth\"\n",
    "        \n",
    "        # Save the results to a text file.\n",
    "        with open(f\"models_traced/generation_{gen}/results_model_{self.idx}.txt\", \"w\") as f:\n",
    "            f.write(\"Test Results:\\n\")\n",
    "            for key, value in self.results[0].items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        # Prepare dummy input from dm.input_shape\n",
    "        input_shape = self.dm.input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            input_shape = (1,) + input_shape\n",
    "        device = next(self.LM.parameters()).device\n",
    "        example_input = torch.randn(*input_shape).to(device)\n",
    "        \n",
    "        self.LM.eval()  # set the model to evaluation mode\n",
    "        \n",
    "        if save_torchscript:\n",
    "            traced_model = torch.jit.trace(self.LM.model, example_input)\n",
    "            traced_model.save(ts_save_path)\n",
    "            print(f\"Scripted (TorchScript) model saved at {ts_save_path}\")\n",
    "        \n",
    "        if save_standard:\n",
    "            # Retrieve architecture code from the individual.\n",
    "            arch_code = self.population[self.idx].architecture\n",
    "            save_dict = {\"state_dict\": self.LM.model.state_dict()}\n",
    "            if arch_code is not None:\n",
    "                save_dict[\"architecture_code\"] = arch_code\n",
    "            torch.save(save_dict, std_save_path)\n",
    "            print(f\"Standard model saved at {std_save_path}\")\n",
    "\n",
    "\n",
    "from myFit import FitnessEvaluator\n",
    "evaluator = FitnessEvaluator()\n",
    "\n",
    "# Train the models in the population           \n",
    "for idx in range(len(pop)): \n",
    "    nt = NASTrainer(population=pop, idx=idx, dm=dm, lr=1e-3, max_epochs=2)\n",
    "    nt.train()\n",
    "    nt.save_model()\n",
    "    \n",
    "    \n",
    "    # Update the population with the results from the model training\n",
    "    fps = nt.results[0]['fps']\n",
    "    metric = nt.results[0]['test_mcc']\n",
    "    ####\n",
    "    pop[idx].iou = nt.results[0]['test_mcc']\n",
    "    pop[idx].fps = nt.results[0]['fps']\n",
    "    \n",
    "    pop[idx].fitness = evaluator.weighted_sum_exponential(fps, metric)\n",
    "    \n",
    "    pop.df.loc[idx, 'Fitness'] = pop[idx].fitness\n",
    "    pop.df.loc[idx, 'Metric'] = pop[idx].iou\n",
    "    pop.df.loc[idx, 'FPS'] = pop[idx].fps\n",
    "    \n",
    "    pop.save_dataframe()\n",
    "    pop.save_population()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making new generation, some important things:\n",
    "\n",
    "- check model size below the thresh when creating new child, otherwise go for another tentative.\n",
    "- retain best K model at each generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "# new_pop = generate_new_population(pop, mating_pool_cutoff, mutation_probability, k_best=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_population[2].parsed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[2].parsed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" * 20)\n",
    "print(f\"*** GENERATION {t} ***\")\n",
    "new_population = []\n",
    "\n",
    "# Create a mating pool\n",
    "mating_pool = population[:int(np.floor(mating_pool_cutoff * len(population)))].copy()\n",
    "for i in range(int(np.ceil((1 - mating_pool_cutoff) * len(population)))):\n",
    "    temp_individual = classes.Individual(max_layers=max_layers)\n",
    "    mating_pool.append(temp_individual)\n",
    "\n",
    "# Coupling and mating\n",
    "couple_i = 0\n",
    "while couple_i < len(mating_pool):\n",
    "    parents = [mating_pool[couple_i], mating_pool[couple_i + 1]]\n",
    "    children = single_point_crossover(parents=parents)\n",
    "    children = mutation(children=children, mutation_probability=mutation_probability )\n",
    "    new_population = new_population + children\n",
    "    couple_i += 2\n",
    "\n",
    "# Update the population\n",
    "population = new_population.copy()\n",
    "for i in population:\n",
    "    i.architecture = i.chromosome2architecture(i.chromosome)\n",
    "population = remove_duplicates(population=population, max_layers=max_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved TorchScript model and test with a dummy input.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_path = \"model_and_architecture.pt\"\n",
    "loaded_model = torch.jit.load(save_path, map_location=device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Ensure input is moved to the correct device\n",
    "example_input = torch.randn(1, *dm.input_shape).to(device)\n",
    "example_input = example_input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(example_input)\n",
    "print(\"Output from the loaded model:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
