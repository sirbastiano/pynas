{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c5dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install setuptools==67.0.0\n",
    "#!pip install .\n",
    "#!pip install rasterio\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e372ac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 20:57:14.008479: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-03 20:57:14.024784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-03 20:57:14.045058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-03 20:57:14.051268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-03 20:57:14.065774: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-03 20:57:14.957417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import boto3\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import rasterio\n",
    "import keras.utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebd8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "image_list = []\n",
    "\n",
    "bucket_region_name = 'eu-west-2'\n",
    "\n",
    "folderpath_p = \"orbital-ai/Phisat/BurntArea/global/TestFolder/\"\n",
    "s3 = boto3.client('s3', region_name = bucket_region_name)\n",
    "bucket_name ='datasets-open'\n",
    "bucket = s3.list_objects(Bucket = bucket_name, Prefix = \"orbital-ai/Phisat/BurntArea/global/TestFolder/\")\n",
    "for obj in bucket['Contents']:\n",
    "    image_list.append(obj['Key'])\n",
    "\n",
    "image_list.pop(0)\n",
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a8fe6",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e74bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.14014238119125366\n",
    "std = 0.09054151177406311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08009e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Images:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "preprocessed_image_files = []\n",
    "\n",
    "for image_file in tqdm(image_list, total=len(image_list), desc='Processing Images'):\n",
    "    images_file = \"s3://datasets-open/\" + image_file\n",
    "    \n",
    "    with rasterio.open(images_file) as image_src:\n",
    "        bands_to_read = [1,2,3,5,6,7,8]\n",
    "        image = image_src.read(bands_to_read)\n",
    "        image = np.transpose(image)\n",
    "        image = (image - mean) / std\n",
    "        image = 1 / (1 + np.exp(-image))\n",
    "        preprocessed_image_files.append(image)\n",
    "               \n",
    "preprocessed_image_array = np.array(preprocessed_image_files, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_and_pad_images(image, tile_size, stride):\n",
    "    padded_images = []\n",
    "\n",
    "    rows, cols, channels = image.shape\n",
    "    mask_size = rows * cols\n",
    "    total_tiles = 0\n",
    "\n",
    "    num_tiles = math.ceil(rows / tile_size) * math.ceil(cols / tile_size)\n",
    "    total_tiles += num_tiles\n",
    "\n",
    "    if rows >= tile_size and cols >= tile_size:\n",
    "        for y in range(0, rows, stride):\n",
    "            for x in range(0, cols, stride):\n",
    "\n",
    "                tile_start_y = min(tile_size, (rows - y))\n",
    "                tile_start_x = min(tile_size, (cols - x))\n",
    "\n",
    "                start_y = y - (tile_size - tile_start_y)\n",
    "                start_x = x - (tile_size - tile_start_x)\n",
    "\n",
    "                tile_image = image[start_y:start_y + tile_size, start_x:start_x + tile_size, :]          \n",
    "\n",
    "                padded_images.append(tile_image)\n",
    "\n",
    "    return padded_images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16896045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_tf_tiles(padded_images, original_image_shape, tile_size, stride):\n",
    "    rows, cols,_ = original_image_shape\n",
    "    num_rows = (rows - 1) // stride + 1\n",
    "    num_cols = (cols - 1) // stride + 1\n",
    "    channels = padded_images[0].shape[-1]\n",
    "    \n",
    "    stitched_image = np.zeros((rows, cols, channels), dtype=np.float32)\n",
    "    stitched_count = np.zeros((rows, cols), dtype=np.float32)\n",
    "    one = np.ones_like(padded_images[0][:,:,0])\n",
    "    \n",
    "    tile_count = 0\n",
    "\n",
    "    for iy in range(0, num_rows):\n",
    "        start_y = iy * stride\n",
    "        end_y = start_y + tile_size\n",
    "        if end_y > rows:\n",
    "            end_y = rows\n",
    "            start_y = rows - tile_size\n",
    "\n",
    "        for ix in range(num_cols):\n",
    "            start_x = ix * stride\n",
    "            end_x = start_x + tile_size\n",
    "            if end_x > cols:\n",
    "                end_x = cols\n",
    "                start_x = cols - tile_size\n",
    "\n",
    "            tile_image = padded_images[tile_count]\n",
    "            stitched_image[start_y:end_y, start_x:end_x, :] += tile_image\n",
    "            stitched_count[start_y:end_y, start_x:end_x] += one\n",
    "            tile_count += 1\n",
    "\n",
    "\n",
    "    #stitched_image /= stitched_count\n",
    "    for c in range(channels):\n",
    "        stitched_image[:, :, c] /= stitched_count\n",
    "\n",
    "    return stitched_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62972032",
   "metadata": {},
   "source": [
    "# PyTorch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = []\n",
    "for file in preprocessed_image_files:\n",
    "    images = slice_and_pad_images(file, tile_size = 256, stride= 256)\n",
    "    images_list.append(images)\n",
    "    \n",
    "print(len(images_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5968a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the model\n",
    "loaded_model = torch.load('Models/model-1.pt')\n",
    "loaded_model = loaded_model.cuda()\n",
    "loaded_model.eval()\n",
    "\n",
    "# Iterate over each set of image tiles\n",
    "predicted_tiles_list = []\n",
    "\n",
    "for image_tiles in tqdm(images_list, desc='Predicting on Tiles'):\n",
    "    predicted_tiles = []\n",
    "    \n",
    "    for tile in image_tiles:\n",
    "        # Convert the tile to a tensor and add batch dimension\n",
    "        tile_tensor = torch.tensor(tile).float().permute(2, 0, 1).unsqueeze(0).cuda()  # Assuming GPU usage\n",
    "        \n",
    "        # Model prediction\n",
    "        with torch.no_grad():\n",
    "            logits = loaded_model(tile_tensor)\n",
    "            #print(f\"Logits shape: {logits.shape}\")\n",
    "            #print(f\"Logits data type: {logits.dtype}\")\n",
    "            #print(f\"Logits first channel value range: min={logits[0, 0, :, :].min().item()}, max={logits[0, 0, :, :].max().item()}\")\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            #print(f\"Probabilities shape after softmax: {probabilities.shape}\")\n",
    "            #print(f\"Probabilities data type: {probabilities.dtype}\")\n",
    "            #print(f\"Probabilities first channel value range: min={probabilities[0, 0, :, :].min().item()}, max={probabilities[0, 0, :, :].max().item()}\")\n",
    "            \n",
    "            # Get the predicted class by using argmax\n",
    "            prediction = torch.argmax(probabilities, dim=1)  # Should result in shape [batch_size, height, width]\n",
    "            #print(f\"Prediction shape after argmax: {prediction.shape}\")\n",
    "            #print(f\"Prediction data type: {prediction.dtype}\")\n",
    "            #print(f\"Prediction unique values: {torch.unique(prediction[0])}\")  # Assuming only one image in the batch\n",
    "        \n",
    "        # Remove the batch dimension and convert back to numpy\n",
    "        prediction = prediction.squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Append the prediction to the list of predicted tiles\n",
    "        predicted_tiles.append(prediction)\n",
    "    \n",
    "    predicted_tiles_list.append(predicted_tiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of tiles to display\n",
    "num_tiles_to_display = 5\n",
    "\n",
    "# Flatten the list of lists to make it easier to sample randomly\n",
    "flattened_images = [tile for sublist in images_list for tile in sublist]\n",
    "flattened_predictions = [tile for sublist in predicted_tiles_list for tile in sublist]\n",
    "\n",
    "# Sample 10 random tiles and their corresponding masks\n",
    "random_indices = random.sample(range(len(flattened_images)), num_tiles_to_display)\n",
    "\n",
    "# Plotting the tiles and their masks\n",
    "plt.figure(figsize=(20, 40))  # Increase figure size for larger images\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Select channels 4, 3, and 1 for RGB-like display\n",
    "    selected_channels = flattened_images[idx][:, :, [3, 2, 0]]\n",
    "    \n",
    "    # Original tile\n",
    "    plt.subplot(num_tiles_to_display, 2, 2 * i + 1)\n",
    "    plt.imshow(selected_channels)\n",
    "    plt.title(f\"Tile {idx+1}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Corresponding output mask\n",
    "    plt.subplot(num_tiles_to_display, 2, 2 * i + 2)\n",
    "    plt.imshow(flattened_predictions[idx], cmap='gray')\n",
    "    plt.title(f\"Mask {idx+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stitch_tiles(padded_images, original_image_shape, tile_size, stride):\n",
    "    rows, cols = original_image_shape[:2]\n",
    "    channels = 1  # Since this is single-channel data\n",
    "    num_rows = (rows - 1) // stride + 1\n",
    "    num_cols = (cols - 1) // stride + 1\n",
    "    \n",
    "    stitched_image = np.zeros((rows, cols), dtype=np.float32)\n",
    "    stitched_count = np.zeros((rows, cols), dtype=np.float32)\n",
    "    one = np.ones((tile_size, tile_size), dtype=np.float32)\n",
    "    \n",
    "    tile_count = 0\n",
    "\n",
    "    for iy in range(num_rows):\n",
    "        start_y = iy * stride\n",
    "        end_y = start_y + tile_size\n",
    "        if end_y > rows:\n",
    "            end_y = rows\n",
    "            start_y = rows - tile_size\n",
    "\n",
    "        for ix in range(num_cols):\n",
    "            start_x = ix * stride\n",
    "            end_x = start_x + tile_size\n",
    "            if end_x > cols:\n",
    "                end_x = cols\n",
    "                start_x = cols - tile_size\n",
    "\n",
    "            tile_image = padded_images[tile_count]\n",
    "\n",
    "            # Adding weighted sum for overlap smoothing\n",
    "            stitched_image[start_y:end_y, start_x:end_x] += tile_image[:end_y-start_y, :end_x-start_x]\n",
    "            stitched_count[start_y:end_y, start_x:end_x] += one[:end_y-start_y, :end_x-start_x]\n",
    "            tile_count += 1\n",
    "\n",
    "    # Normalize the stitched image by the stitch count to smooth overlaps\n",
    "    stitched_image /= stitched_count\n",
    "\n",
    "    return stitched_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch the predicted tiles back together\n",
    "stitched_images = []\n",
    "\n",
    "print(len(predicted_tiles_list))\n",
    "for i, predicted_tiles in enumerate(predicted_tiles_list):\n",
    "    stitched_image = stitch_tiles(predicted_tiles, preprocessed_image_files[i].shape, tile_size=256, stride=256)\n",
    "    stitched_images.append(stitched_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "\n",
    "# Define colormap\n",
    "colormap = {\n",
    "    0: [0, 0, 0],       # Class 0 (black)\n",
    "    1: [255, 0, 0],     # Class 1 (red)\n",
    "    2: [0, 255, 0],     # Class 2 (green)\n",
    "    3: [0, 0, 255]      # Class 3 (blue)\n",
    "}\n",
    "\n",
    "# Directory containing the original model outputs\n",
    "original_outputs_dir = 'Original-Outputs/'\n",
    "\n",
    "for i, (imgs, predictions) in enumerate(zip(preprocessed_image_array, stitched_images)):\n",
    "    # Ensure imgs is a numeric array\n",
    "    imgs = np.array(imgs, dtype=np.float32)\n",
    "\n",
    "    # Create the RGB image using channels 4, 3, 1 from the original image (assuming they are RGB)\n",
    "    rgb_img = np.stack((imgs[:, :, 4], imgs[:, :, 3], imgs[:, :, 1]), axis=-1)\n",
    "\n",
    "    # Convert rgb_img to uint8 if necessary (assuming the range is 0-1 for float32)\n",
    "    rgb_img = (rgb_img * 255).astype(np.uint8)\n",
    "\n",
    "    # Initialize an RGB array for the predictions\n",
    "    rgb_prediction = np.zeros((predictions.shape[0], predictions.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Apply the colormap to the prediction\n",
    "    for class_label in np.unique(predictions):\n",
    "        if class_label in colormap:  # Ensure the class_label exists in colormap\n",
    "            mask = predictions == class_label\n",
    "            rgb_prediction[mask] = colormap[class_label]\n",
    "\n",
    "    # Load the corresponding original model output image\n",
    "    original_output_path = os.path.join(original_outputs_dir, f'Original_output-{i+1}.tif')\n",
    "    original_output_img = imread(original_output_path)\n",
    "\n",
    "    # Plot the original image, the predicted segmentation, and the original model output\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    print(f\"Image: {i+1}\")\n",
    "    axes[0].imshow(rgb_img)\n",
    "    axes[0].set_title('Original Scene')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(rgb_prediction)\n",
    "    axes[1].set_title('Predicted Scene')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(original_output_img)\n",
    "    axes[2].set_title('Original Model Output')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc059bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
