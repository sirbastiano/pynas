{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pynas.core.population import Population\n",
    "from datasets.RawClassifier.loader import RawClassifierDataModule\n",
    "\n",
    "# Define dataset module\n",
    "root_dir = '/Data_large/marine/PythonProjects/OtherProjects/lpl-PyNas/data/RawClassifier'\n",
    "dm = RawClassifierDataModule(root_dir, batch_size=4, num_workers=2, transform=None)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "def setting():\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # Logging\n",
    "    logs_directory = str(config['GA']['logs_dir_GA'])\n",
    "    # Torch stuff\n",
    "    seed = config.getint(section='Computation', option='seed')\n",
    "    pl.seed_everything(seed=seed, workers=True)  # For reproducibility\n",
    "    torch.set_float32_matmul_precision(\"medium\")  # to make lightning happy\n",
    "setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "max_layers = 3\n",
    "max_iter = int(config['GA']['max_iterations'])\n",
    "# GA parameters\n",
    "n_individuals = int(config['GA']['population_size'])\n",
    "mating_pool_cutoff = float(config['GA']['mating_pool_cutoff'])\n",
    "mutation_probability = float(config['GA']['mutation_probability'])\n",
    "\n",
    "pop = Population(n_individuals=20, max_layers=max_layers, dm=dm, max_parameters=400_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.initial_poll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.train_generation(task='classification', lr=0.001, epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.evolve(mating_pool_cutoff=mating_pool_cutoff, mutation_probability=0.85, k_best=1, n_random=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataframe Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_dataframe method in the Population class is used to retrieve the stored results \n",
    "or evaluation metrics from the training and evolution process of the models.\n",
    "By calling pop.load_dataframe(9), it is expected that the method will load data (e.g., performance, \n",
    "loss values, or architectural configurations) that was saved during the process.\n",
    "\n",
    "This data can then be used for analysis, visualization, or further processing, providing insights \n",
    "into the model's training dynamics or the overall evolutionary process. Make sure that the\n",
    "index passed to load_dataframe (in this case, 9) corresponds to the correct set of results you intend to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.load_dataframe(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Using the evaluated and saved model. We use the traced pytroch model (.pt) to load and execute inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved TorchScript model and test with a dummy input.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_path = \"model_and_architecture.pt\"\n",
    "loaded_model = torch.jit.load(save_path, map_location=device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Ensure input is moved to the correct device\n",
    "example_input = torch.randn(1, *dm.input_shape).to(device)\n",
    "example_input = example_input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(example_input)\n",
    "print(\"Output from the loaded model:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
